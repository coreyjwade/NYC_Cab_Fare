{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 150000\n",
    "NODES = [100,100,50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import model_from_json\n",
    "from sklearn.externals import joblib\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_dataFrame(file_name, subset=True, nrows=ROWS):\n",
    "    if subset:\n",
    "        df = pd.read_csv(file_name, nrows=nrows, parse_dates=['pickup_datetime'])\n",
    "    else:\n",
    "        df = pd.read_csv(file_name, parse_dates=['pickup_datetime'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xtest_ytest(df):\n",
    "    y_test = df['key']\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    X_test = df.drop('key', axis=1)\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    print('dropping nan:', len(df))\n",
    "    df = df.dropna(axis=0, subset=['dropoff_latitude'])\n",
    "    df = df.drop('key', axis=1)\n",
    "    print('nan dropped:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_US(df):\n",
    "    # Choose cab rides whose pickup and dropoff are the US Mainland\n",
    "    # Declare constants\n",
    "    latmin = 5.496100\n",
    "    latmax = 71.538800\n",
    "    longmin = -124.482003\n",
    "    longmax = -66.885417\n",
    "\n",
    "    # Create dataframe with correct coordinates\n",
    "    df = df[((((df['pickup_longitude']<=longmax) & (df['pickup_longitude']>=longmin)) & ((df['pickup_latitude']<=latmax) & (df['pickup_latitude']>=latmin)))) & ((((df['dropoff_longitude']<=longmax) & (df['dropoff_longitude']>=longmin)) & ((df['dropoff_latitude']<=latmax) & (df['dropoff_latitude']>=latmin))))]\n",
    "    \n",
    "    print('US Mainland Only dropped:', len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_NYC(df):\n",
    "    # Find cab rides whose pickup or dropoff are within NYC boundaries\n",
    "    # Declare constants\n",
    "    latmin = 40.477399\n",
    "    latmax = 40.917577\n",
    "    longmin = -74.259090\n",
    "    longmax = -73.700272\n",
    "\n",
    "    # Create dataframe with correct coordinates\n",
    "    df = df[((((df['pickup_longitude']<=longmax) & (df['pickup_longitude']>=longmin)) & ((df['pickup_latitude']<=latmax) & (df['pickup_latitude']>=latmin)))) | ((((df['dropoff_longitude']<=longmax) % (df['dropoff_longitude']>=longmin)) & ((df['dropoff_latitude']<=latmax) & (df['dropoff_latitude']>=latmin))))]\n",
    "    \n",
    "    print('NYC Taxis Only:', len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_Riders(df, num=7):\n",
    "    # Only choose cabs between 1 and num riders\n",
    "    df = df[(df['passenger_count'] <= num) & (df['passenger_count'] > 0)]\n",
    "    print('Max Passengers 7:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance(df):\n",
    "\n",
    "    # Define coordinates (x,y)\n",
    "    x1 = df['pickup_latitude']\n",
    "    y1 = df['pickup_longitude']\n",
    "    x2 = df['dropoff_latitude']\n",
    "    y2 = df['dropoff_longitude']\n",
    "\n",
    "    # Create Euclidean Distrance column\n",
    "    df['euclidean_distance'] = np.sqrt((y2-y1)**2 + (x2-x1)**2)\n",
    "\n",
    "    # Create Taxicab Distance column\n",
    "    df['taxicab_distance'] = np.abs(y2-y1) + np.abs(x2-x1)\n",
    "\n",
    "    # Convert to miles\n",
    "    df['euclidean_distance'] = df['euclidean_distance'] * 69\n",
    "    df['taxicab_distance'] = df['taxicab_distance'] * 69\n",
    "    \n",
    "    print('Distance Columns added...')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_Fare(df):\n",
    "    # Eliminate unrealistic plots\n",
    "    df = df[df['fare_amount'] >= (df['euclidean_distance'] * 2 + 2.5)]\n",
    "    print('Min fares dropped:', len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_Fare(df):\n",
    "    df = df[(df['fare_amount'] <= (df['taxicab_distance'] * 48 + 16)) | (df['fare_amount'] <= 56)]\n",
    "    print('Max fares dropped:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_distance(df):\n",
    "    # Elminate fares that traveled no distance\n",
    "    df = df[df['euclidean_distance']>0]\n",
    "    print('No distance dropped:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_elimination(df):\n",
    "    df = clean_data(df)\n",
    "    df = lat_lon_US(df)\n",
    "    df = lat_lon_NYC(df)\n",
    "    df = max_Riders(df)\n",
    "    df = add_distance(df)\n",
    "    df = min_Fare(df)\n",
    "    df = max_Fare(df)\n",
    "    df = no_distance(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_train, y_train Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_y(df):\n",
    "    X = df.drop('fare_amount', axis=1)\n",
    "    y = df['fare_amount'].copy()\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garbage Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32698"
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of accumulated garbage\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Time_units(df):\n",
    "    \n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['minute'] = df['pickup_datetime'].dt.minute\n",
    "    df['second'] = df['pickup_datetime'].dt.second\n",
    "    df['dayofweek'] = df['pickup_datetime'].dt.dayofweek\n",
    "    \n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "    dr = pd.date_range(start='2009-01-01', end='2015-12-31')\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "    df['holiday'] = df['pickup_datetime'].dt.date.astype('datetime64').isin(holidays)\n",
    "    \n",
    "    df = df.drop('pickup_datetime', axis=1)\n",
    "\n",
    "    df['total_seconds'] = 3600 * df['hour'] + 60 * df['minute'] + df['second']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Time_columns(df):\n",
    "    \n",
    "    def morning_rush(row):\n",
    "        if ((row['hour'] in [6,7,8,9]) & (row['dayofweek'] in [0,1,2,3,4])) & (not row['holiday']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['morning_rush'] = df.apply(morning_rush, axis=1)\n",
    "\n",
    "    def night_charge(row):\n",
    "        if row['hour'] in [20,21,22,23,24,1,2,3,4,5,6]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['night_charge'] = df.apply(night_charge, axis=1)\n",
    "\n",
    "    def weekday_surcharge(row):\n",
    "        if ((row['hour'] in [16,17,18,19,20]) & (row['dayofweek'] in [0,1,2,3,4])) & (not row['holiday']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['weekday_surcharge'] = df.apply(weekday_surcharge, axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Time(df):\n",
    "    df = add_Time_units(df)\n",
    "    df = add_Time_columns(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define line from two points and a provided column\n",
    "def two_points_line(a, b, column):\n",
    "        \n",
    "    # Case when y-values are the same\n",
    "    if b[1]==a[1]:\n",
    "        \n",
    "        # Slope defaults to 0\n",
    "        slope = 0\n",
    "        \n",
    "    # Case when x-values are the same\n",
    "    elif b[0]==a[0]:\n",
    "        \n",
    "        # Case when max value is less than 999999999\n",
    "        if column.max() < 999999999:\n",
    "            \n",
    "            # Add 999999999 to max value\n",
    "            slope = column.max() + 999999999\n",
    "        \n",
    "        # All other cases\n",
    "        else:\n",
    "            \n",
    "            # Multiply max value by itself (greater than 999999999)\n",
    "            slope = column.max() * column.max()\n",
    "    \n",
    "    # When x-values and y-values are not 0\n",
    "    else:\n",
    "        \n",
    "        # Use standard slope formula\n",
    "        slope = (b[1] - a[1])/(b[0]-a[0])\n",
    "    \n",
    "    \n",
    "    # Equation for y-intercept (solving y=mx+b for b)\n",
    "    y_int = a[1] - slope * a[0]\n",
    "    \n",
    "    # Return slope and y-intercept\n",
    "    return slope, y_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_cols(df):\n",
    "    \n",
    "    upper_right = (-73.929224, 40.804328)\n",
    "    bottom_right = (-73.980036, 40.710706)\n",
    "    bottom_left = (-74.054880, 40.681292)\n",
    "    upper_left = (-73.966303, 40.830050)\n",
    "\n",
    "    m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "    m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "    m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "    m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "    def manhattan(row):\n",
    "        if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "        (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['pickup_latitude'] >= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "        (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))) & (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "        (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "        (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    df['manhattan'] = df.apply(manhattan, axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newark_cols(df):\n",
    "    \n",
    "    upper_right = (-74.107867, 40.718282)\n",
    "    bottom_right = (-74.143665, 40.654673)\n",
    "    bottom_left = (-74.250524, 40.698436)\n",
    "    upper_left = (-74.171983, 40.792347)\n",
    "\n",
    "    m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "    m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "    m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "    m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "    def newark(row):\n",
    "        if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "        (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['pickup_latitude'] >= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "        (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))) | (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "        (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "        (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    df['newark'] = df.apply(newark, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jkf_cols(df):\n",
    "    \n",
    "    upper_right = (-73.789700, 40.663781)\n",
    "    bottom_right = (-73.762112, 40.633567)\n",
    "    bottom_left = (-73.818920, 40.642250)\n",
    "    upper_left = (-73.804656, 40.664858)\n",
    "\n",
    "    m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "    m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "    m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "    m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "    def jfk(row):\n",
    "        if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "        (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['pickup_latitude'] <= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "        (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))) | (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "        (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "        (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    df['jfk'] = df.apply(jfk, axis=1)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_locations(df):\n",
    "    df = manhattan_cols(df)\n",
    "    df = jkf_cols(df)\n",
    "    df = newark_cols(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cols(df):\n",
    "    df = add_Time(df)\n",
    "    df = add_locations(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_predictor_cols(df, cols=['month', 'year', 'dayofweek', 'total_seconds', 'morning_rush', 'night_charge', 'weekday_surcharge', 'manhattan', 'jfk', 'newark', 'passenger_count','euclidean_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']):\n",
    "    X = df[cols]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_cols(X):\n",
    "    X = one_Hot_Encoder(X, X['month'])\n",
    "    del X['month']\n",
    "    X = one_Hot_Encoder(X, X['dayofweek'], month=False)\n",
    "    del X['dayofweek']\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_Hot_Encoder(X, col, month=True): \n",
    "    encoder = OneHotEncoder()\n",
    "    hot_array = encoder.fit_transform(np.array(col).reshape(-1,1)).toarray()\n",
    "    hot_df = pd.DataFrame(hot_array)\n",
    "    if month:\n",
    "        hot_df.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    else:\n",
    "        hot_df.columns = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "    new_df = X.join(hot_df)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(X):\n",
    "    X = X.reset_index(drop=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pipeline(test_set=False):\n",
    "    \n",
    "    if test_set:\n",
    "        df = file_to_dataFrame('test.csv')\n",
    "        X, y = make_Xtest_ytest(df)\n",
    "        X = add_distance(X)\n",
    "    \n",
    "    else:\n",
    "        df = file_to_dataFrame('train.csv')\n",
    "        df = row_elimination(df)\n",
    "        X, y = make_X_y(df)\n",
    "    \n",
    "    X = reset_index(X)\n",
    "    X = add_cols(X)\n",
    "    X = choose_predictor_cols(X)\n",
    "    X = one_hot_cols(X)\n",
    "    X = min_max_scaler(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression():\n",
    "        \n",
    "    print('Length of X:', len(X_train))\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    scores = cross_val_score(lr_model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse = np.sqrt(-scores)\n",
    "    print('Lin reg train rmse:', rmse)\n",
    "    print('Lin reg train mean:', rmse.mean())\n",
    "    print('Lin reg train std:', rmse.std())\n",
    "    \n",
    "    joblib.dump(lr_model, 'lr_model.pkl') \n",
    "    \n",
    "    print('Linear Regression model saved as \"lr_model.pkl\"')\n",
    "    \n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_random_forest_tuner(X,y):\n",
    "        \n",
    "    param_grid = [\n",
    "        {'n_estimators': [75, 100, 250, 500, 750, 1000, 15000], 'max_features': [5, 10, 15, 20, 25]}, \n",
    "    ]\n",
    "    \n",
    "    forest_reg = RandomForestRegressor()\n",
    "    \n",
    "    forest_reg_tuned = RandomSearchCV(forest_reg, param_grid, n_iter=6, cv=3, \n",
    "                                    scoring='neg_mean_squared_error')\n",
    "    \n",
    "    forest_reg_tuned.fit(X,y)\n",
    "    \n",
    "    # Print the tuned parameters and score\n",
    "    print(\"Tuned Random Forest Parameters: {}\".format(forest_reg_tuned.best_params_))\n",
    "    \n",
    "    scores = cross_val_score(forest_reg_tuned, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "    \n",
    "    display_scores('Random Forest', scores)\n",
    "    \n",
    "    return forest_reg_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_tuner(X,y):\n",
    "        \n",
    "    param_grid = [\n",
    "        {'n_estimators': [100, 500, 1000], 'max_features': [10]}, \n",
    "    ]\n",
    "    \n",
    "    forest_reg = RandomForestRegressor()\n",
    "    \n",
    "    forest_reg_tuned = GridSearchCV(forest_reg, param_grid, cv=3, \n",
    "                                    scoring='neg_mean_squared_error')\n",
    "    \n",
    "    forest_reg_tuned.fit(X,y)\n",
    "    \n",
    "    # Print the tuned parameters and score\n",
    "    print(\"Tuned Random Forest Parameters: {}\".format(forest_reg_tuned.best_params_))\n",
    "    \n",
    "    scores = cross_val_score(forest_reg_tuned, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "    \n",
    "    display_scores('Random Forest', scores)\n",
    "    \n",
    "    return forest_reg_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(title, scores):\n",
    "    rmse = np.sqrt(-scores)\n",
    "    print(title, ' rmse scores:', rmse)\n",
    "    print(title, ' mean score:', rmse.mean())\n",
    "    print(title, ' std:', rmse.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest():\n",
    "    \n",
    "    rf_model = RandomForestRegressor(max_features=10, n_estimators=50)\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    scores = cross_val_score(rf_model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    display_scores('Random Forest', scores)\n",
    "    \n",
    "    joblib.dump(rf_model, 'rf_model.pkl') \n",
    "    \n",
    "    print('Random forest model saves as \"rf_model.pkl\"')\n",
    "        \n",
    "    return rf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_regression_test requires \"from sklearn.model_selection import train_test_split\"\n",
    "def deep_learning(nodes=NODES, batch_size=32, activation='relu', optimizer='adam', loss='mean_squared_error'):\n",
    "        \n",
    "    X, X_check, y, y_check = train_test_split(X_train, y_train)\n",
    "    \n",
    "    # Save the number of columns in predictors: n_cols\n",
    "    n_cols = X.shape[1]\n",
    "\n",
    "    # Set up the model: model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add the first layer\n",
    "    model.add(Dense(nodes[0], activation=activation, input_shape=(n_cols,)))\n",
    "    \n",
    "    # Add addition layers\n",
    "    for i in range(len(nodes)-1):\n",
    "        model.add(Dense(nodes[i+1], activation=activation, kernel_constraint=maxnorm(3)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    # Define early_stopping_monitor\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X, y, validation_split=0.05, epochs=30, batch_size=batch_size, callbacks=[early_stopping_monitor])\n",
    "\n",
    "    # Get score for predictions\n",
    "    score = model.evaluate(X_check, y_check)\n",
    "    \n",
    "    # Get root mean squared error\n",
    "    rmse = np.sqrt(score)\n",
    "    \n",
    "    # Return root mean squared error\n",
    "    print(rmse)\n",
    "    \n",
    "    save_keras_model(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keras_model(model):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"dl_model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved deep learning model as 'dl_model.json'\")\n",
    "    return model\n",
    "  \n",
    "def open_keras_model(file):\n",
    "    # load json and create model\n",
    "    json_file = open(file, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_model(saved_model, keras=False):\n",
    "    if keras:\n",
    "        model = open_keras_model(saved_model)\n",
    "    else:\n",
    "        model = joblib.load(saved_model)\n",
    "    return model\n",
    "\n",
    "def kaggle_submit(saved_model, keras=False):\n",
    "    if keras:\n",
    "        model = open_model(saved_model, keras=True)\n",
    "    else:\n",
    "        model = open_model(saved_model)\n",
    "    y_pred = model.predict(X_test)\n",
    "    submit_to_kaggle(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping nan: 150000\n",
      "nan dropped: 149999\n",
      "US Mainland Only dropped: 146911\n",
      "NYC Taxis Only: 146732\n",
      "Max Passengers 7: 146192\n",
      "Distance Columns added...\n",
      "Min fares dropped: 141891\n",
      "Max fares dropped: 141807\n",
      "No distance dropped: 140311\n",
      "Distance Columns added...\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = my_pipeline()\n",
    "X_test, y_test = my_pipeline(test_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 140311\n",
      "Lin reg train rmse: [3.56801881 3.75088332 3.76600961 3.56703533 3.63740094]\n",
      "Lin reg train mean: 3.6578696019310613\n",
      "Lin reg train std: 0.08612642774919092\n",
      "Linear Regression model saves as \"lr_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                key  fare_amount\n",
      "0       2015-01-27 13:08:24.0000002    10.300781\n",
      "1       2015-01-27 13:08:24.0000003    10.644531\n",
      "2       2011-10-08 11:53:44.0000002     5.871094\n",
      "3       2012-12-01 21:12:12.0000002     7.949219\n",
      "4       2012-12-01 21:12:12.0000003    13.804688\n",
      "5       2012-12-01 21:12:12.0000005    10.320312\n",
      "6       2011-10-06 12:10:20.0000001     7.003906\n",
      "7       2011-10-06 12:10:20.0000003    48.417969\n",
      "8       2011-10-06 12:10:20.0000002    11.814453\n",
      "9       2014-02-18 15:22:20.0000002     8.158203\n",
      "10      2014-02-18 15:22:20.0000003    10.187500\n",
      "11      2014-02-18 15:22:20.0000001    15.187500\n",
      "12      2010-03-29 20:20:32.0000002     4.216797\n",
      "13      2010-03-29 20:20:32.0000001     6.328125\n",
      "14      2011-10-06 03:59:12.0000002     7.890625\n",
      "15      2011-10-06 03:59:12.0000001    12.484375\n",
      "16      2012-07-15 16:45:04.0000006     5.542969\n",
      "17      2012-07-15 16:45:04.0000002     9.509766\n",
      "18      2012-07-15 16:45:04.0000003     5.984375\n",
      "19      2012-07-15 16:45:04.0000004     5.648438\n",
      "20      2014-10-29 02:09:56.0000001    10.406250\n",
      "21     2014-06-14 13:39:00.00000010     9.826172\n",
      "22     2014-06-14 13:39:00.00000060     8.197266\n",
      "23     2014-06-14 13:39:00.00000087     9.281250\n",
      "24     2014-06-14 13:39:00.00000050    16.191406\n",
      "25      2014-06-14 13:39:00.0000003     8.246094\n",
      "26    2014-06-14 13:39:00.000000158    35.160156\n",
      "27     2014-06-14 13:39:00.00000015    21.941406\n",
      "28     2014-06-14 13:39:00.00000073     7.750000\n",
      "29     2014-06-14 13:39:00.00000077    15.007812\n",
      "...                             ...          ...\n",
      "9884   2013-09-25 22:00:00.00000060    30.660156\n",
      "9885  2013-09-25 22:00:00.000000213    15.265625\n",
      "9886  2013-09-25 22:00:00.000000150    19.972656\n",
      "9887   2013-09-25 22:00:00.00000010     9.390625\n",
      "9888  2013-09-25 22:00:00.000000146     5.826172\n",
      "9889   2013-09-25 22:00:00.00000041    12.029297\n",
      "9890  2013-09-25 22:00:00.000000109    11.306641\n",
      "9891  2013-09-25 22:00:00.000000210    16.642578\n",
      "9892  2013-09-25 22:00:00.000000151    10.851562\n",
      "9893  2013-09-25 22:00:00.000000190    15.984375\n",
      "9894  2013-09-25 22:00:00.000000153    11.097656\n",
      "9895  2013-09-25 22:00:00.000000241    23.283203\n",
      "9896  2013-09-25 22:00:00.000000127    10.933594\n",
      "9897    2015-02-20 11:08:29.0000001    15.101562\n",
      "9898    2015-01-12 15:36:37.0000002     7.248047\n",
      "9899    2015-06-07 00:38:14.0000002    19.087891\n",
      "9900    2015-04-12 21:56:22.0000005     8.734375\n",
      "9901    2015-04-10 11:56:54.0000004     9.103516\n",
      "9902    2015-06-25 01:01:46.0000002    15.093750\n",
      "9903    2015-05-29 10:02:42.0000001    10.583984\n",
      "9904    2015-06-30 20:03:50.0000002    39.263672\n",
      "9905    2015-02-27 19:36:02.0000006    24.408203\n",
      "9906    2015-06-15 01:00:06.0000002     6.015625\n",
      "9907    2015-02-03 09:00:58.0000001    29.894531\n",
      "9908    2015-05-19 13:58:11.0000001     9.656250\n",
      "9909    2015-05-10 12:37:51.0000002     9.863281\n",
      "9910    2015-01-12 17:05:51.0000001    11.703125\n",
      "9911    2015-04-19 20:44:15.0000001    44.257812\n",
      "9912    2015-01-31 01:05:19.0000005    18.832031\n",
      "9913    2015-01-18 14:06:23.0000006     8.521484\n",
      "\n",
      "[9914 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "kaggle_submit('lr_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  rmse scores: [2.93958553 3.1945351  3.20662355 2.97817472 3.201462  ]\n",
      "Random Forest  mean score: 3.104076179478646\n",
      "Random Forest  std: 0.11924020203471307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=10, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=40, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                key  fare_amount\n",
      "0       2015-01-27 13:08:24.0000002     12.79500\n",
      "1       2015-01-27 13:08:24.0000003     15.44500\n",
      "2       2011-10-08 11:53:44.0000002      9.15000\n",
      "3       2012-12-01 21:12:12.0000002     10.45250\n",
      "4       2012-12-01 21:12:12.0000003     14.40750\n",
      "5       2012-12-01 21:12:12.0000005     11.94575\n",
      "6       2011-10-06 12:10:20.0000001     10.35500\n",
      "7       2011-10-06 12:10:20.0000003     60.55550\n",
      "8       2011-10-06 12:10:20.0000002     17.90250\n",
      "9       2014-02-18 15:22:20.0000002     13.79750\n",
      "10      2014-02-18 15:22:20.0000003     13.85375\n",
      "11      2014-02-18 15:22:20.0000001     18.09575\n",
      "12      2010-03-29 20:20:32.0000002     11.01625\n",
      "13      2010-03-29 20:20:32.0000001     11.38375\n",
      "14      2011-10-06 03:59:12.0000002     13.10750\n",
      "15      2011-10-06 03:59:12.0000001     15.84625\n",
      "16      2012-07-15 16:45:04.0000006     10.31000\n",
      "17      2012-07-15 16:45:04.0000002     12.03750\n",
      "18      2012-07-15 16:45:04.0000003      9.72500\n",
      "19      2012-07-15 16:45:04.0000004      9.71250\n",
      "20      2014-10-29 02:09:56.0000001     20.68900\n",
      "21     2014-06-14 13:39:00.00000010     12.02500\n",
      "22     2014-06-14 13:39:00.00000060     11.53250\n",
      "23     2014-06-14 13:39:00.00000087     10.82375\n",
      "24     2014-06-14 13:39:00.00000050     18.03575\n",
      "25      2014-06-14 13:39:00.0000003     11.54500\n",
      "26    2014-06-14 13:39:00.000000158     40.37225\n",
      "27     2014-06-14 13:39:00.00000015     26.14725\n",
      "28     2014-06-14 13:39:00.00000073     10.66250\n",
      "29     2014-06-14 13:39:00.00000077     16.47250\n",
      "...                             ...          ...\n",
      "9884   2013-09-25 22:00:00.00000060     39.42225\n",
      "9885  2013-09-25 22:00:00.000000213     17.15625\n",
      "9886  2013-09-25 22:00:00.000000150     32.86275\n",
      "9887   2013-09-25 22:00:00.00000010     11.84625\n",
      "9888  2013-09-25 22:00:00.000000146     20.76750\n",
      "9889   2013-09-25 22:00:00.00000041     15.29100\n",
      "9890  2013-09-25 22:00:00.000000109     16.95875\n",
      "9891  2013-09-25 22:00:00.000000210     20.72750\n",
      "9892  2013-09-25 22:00:00.000000151     16.71375\n",
      "9893  2013-09-25 22:00:00.000000190     33.49075\n",
      "9894  2013-09-25 22:00:00.000000153     16.95875\n",
      "9895  2013-09-25 22:00:00.000000241     34.49300\n",
      "9896  2013-09-25 22:00:00.000000127     16.16375\n",
      "9897    2015-02-20 11:08:29.0000001     15.92500\n",
      "9898    2015-01-12 15:36:37.0000002     11.56125\n",
      "9899    2015-06-07 00:38:14.0000002     23.23700\n",
      "9900    2015-04-12 21:56:22.0000005     12.26375\n",
      "9901    2015-04-10 11:56:54.0000004     13.08375\n",
      "9902    2015-06-25 01:01:46.0000002     27.13575\n",
      "9903    2015-05-29 10:02:42.0000001     11.86625\n",
      "9904    2015-06-30 20:03:50.0000002     46.83225\n",
      "9905    2015-02-27 19:36:02.0000006     38.83100\n",
      "9906    2015-06-15 01:00:06.0000002     11.95750\n",
      "9907    2015-02-03 09:00:58.0000001     39.15550\n",
      "9908    2015-05-19 13:58:11.0000001     12.60625\n",
      "9909    2015-05-10 12:37:51.0000002     10.93375\n",
      "9910    2015-01-12 17:05:51.0000001     17.66375\n",
      "9911    2015-04-19 20:44:15.0000001     51.08575\n",
      "9912    2015-01-31 01:05:19.0000005     23.73500\n",
      "9913    2015-01-18 14:06:23.0000006     12.24000\n",
      "\n",
      "[9914 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "kaggle_submit('rf_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5133332720979047, 'euclidean_distance'),\n",
       " (0.13953042368532476, 'manhattan'),\n",
       " (0.07363370548489193, 'jfk'),\n",
       " (0.06393871525252566, 'dropoff_longitude'),\n",
       " (0.053156706641823516, 'pickup_longitude'),\n",
       " (0.043079284579644266, 'dropoff_latitude'),\n",
       " (0.036569960777515426, 'pickup_latitude'),\n",
       " (0.019605323230097702, 'year'),\n",
       " (0.012460993949790524, 'total_seconds'),\n",
       " (0.00916419075100937, 'newark'),\n",
       " (0.008808938852134011, '15_min_intervals'),\n",
       " (0.002681150543599505, 'passenger_count'),\n",
       " (0.0023097732259236762, 'night_charge'),\n",
       " (0.0014802166748075797, 'Sep'),\n",
       " (0.0014105295197307307, 'Fri'),\n",
       " (0.0013284927157269147, 'Sat'),\n",
       " (0.0012319062149000225, 'Sun'),\n",
       " (0.0012072193884114255, 'Wed'),\n",
       " (0.0012060537037168972, 'Dec'),\n",
       " (0.0011829885879903284, 'Tue'),\n",
       " (0.0011751225329398938, 'Thu'),\n",
       " (0.0010962228693957895, 'Oct'),\n",
       " (0.0010345787523343814, 'Mon'),\n",
       " (0.0009648258025364915, 'Nov'),\n",
       " (0.0009606868604274225, 'May'),\n",
       " (0.0009415374192659018, 'weekday_surcharge'),\n",
       " (0.0008805622533894549, 'Apr'),\n",
       " (0.0008714395302517735, 'Jun'),\n",
       " (0.0008595831259823473, 'Jan'),\n",
       " (0.0008594399656712468, 'Aug'),\n",
       " (0.0008041232452107606, 'Jul'),\n",
       " (0.0007713992209782753, 'Feb'),\n",
       " (0.0007649029956916994, 'morning_rush'),\n",
       " (0.0006957295484557914, 'Mar')]"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = rf_model.feature_importances_\n",
    "sorted(zip(feature_importances, list(X_train)), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103128 samples, validate on 2105 samples\n",
      "Epoch 1/30\n",
      "103128/103128 [==============================] - 8s 73us/step - loss: 21.0207 - val_loss: 9.5682\n",
      "Epoch 2/30\n",
      "103128/103128 [==============================] - 6s 61us/step - loss: 13.5750 - val_loss: 10.1264\n",
      "Epoch 3/30\n",
      "103128/103128 [==============================] - 6s 62us/step - loss: 12.9216 - val_loss: 9.0437\n",
      "Epoch 4/30\n",
      "103128/103128 [==============================] - 6s 61us/step - loss: 12.6124 - val_loss: 9.3169\n",
      "Epoch 5/30\n",
      "103128/103128 [==============================] - 7s 64us/step - loss: 12.3773 - val_loss: 9.1439\n",
      "Epoch 6/30\n",
      "103128/103128 [==============================] - 6s 62us/step - loss: 12.2079 - val_loss: 8.9230\n",
      "Epoch 7/30\n",
      "103128/103128 [==============================] - 7s 64us/step - loss: 12.0149 - val_loss: 9.0606\n",
      "Epoch 8/30\n",
      "103128/103128 [==============================] - 6s 62us/step - loss: 11.8590 - val_loss: 9.3150\n",
      "Epoch 9/30\n",
      "103128/103128 [==============================] - 7s 66us/step - loss: 11.7301 - val_loss: 8.8583\n",
      "Epoch 10/30\n",
      "103128/103128 [==============================] - 7s 64us/step - loss: 11.5533 - val_loss: 9.5262\n",
      "Epoch 11/30\n",
      "103128/103128 [==============================] - 6s 63us/step - loss: 11.4806 - val_loss: 8.9252\n",
      "Epoch 12/30\n",
      "103128/103128 [==============================] - 7s 64us/step - loss: 11.2980 - val_loss: 9.8193\n",
      "35078/35078 [==============================] - 1s 29us/step\n",
      "3.6373370141014076\n",
      "Saved deep learning model as 'dl_model.json'\n"
     ]
    }
   ],
   "source": [
    "NODES = [130,13,130]\n",
    "dl_model = deep_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "                                key  fare_amount\n",
      "0       2015-01-27 13:08:24.0000002    10.799384\n",
      "1       2015-01-27 13:08:24.0000003    11.271863\n",
      "2       2011-10-08 11:53:44.0000002     7.030448\n",
      "3       2012-12-01 21:12:12.0000002     9.589872\n",
      "4       2012-12-01 21:12:12.0000003    15.570109\n",
      "5       2012-12-01 21:12:12.0000005    11.982323\n",
      "6       2011-10-06 12:10:20.0000001     8.677094\n",
      "7       2011-10-06 12:10:20.0000003    42.664478\n",
      "8       2011-10-06 12:10:20.0000002    13.659761\n",
      "9       2014-02-18 15:22:20.0000002     7.811248\n",
      "10      2014-02-18 15:22:20.0000003    10.253995\n",
      "11      2014-02-18 15:22:20.0000001    16.093931\n",
      "12      2010-03-29 20:20:32.0000002     5.989366\n",
      "13      2010-03-29 20:20:32.0000001     7.846018\n",
      "14      2011-10-06 03:59:12.0000002     8.266769\n",
      "15      2011-10-06 03:59:12.0000001    11.906550\n",
      "16      2012-07-15 16:45:04.0000006     7.145779\n",
      "17      2012-07-15 16:45:04.0000002    10.380497\n",
      "18      2012-07-15 16:45:04.0000003     7.344511\n",
      "19      2012-07-15 16:45:04.0000004     7.073923\n",
      "20      2014-10-29 02:09:56.0000001     6.332218\n",
      "21     2014-06-14 13:39:00.00000010    10.434554\n",
      "22     2014-06-14 13:39:00.00000060     8.629930\n",
      "23     2014-06-14 13:39:00.00000087    10.121709\n",
      "24     2014-06-14 13:39:00.00000050    16.930206\n",
      "25      2014-06-14 13:39:00.0000003     8.732541\n",
      "26    2014-06-14 13:39:00.000000158    33.675907\n",
      "27     2014-06-14 13:39:00.00000015    22.442755\n",
      "28     2014-06-14 13:39:00.00000073     7.627091\n",
      "29     2014-06-14 13:39:00.00000077    15.846231\n",
      "...                             ...          ...\n",
      "9884   2013-09-25 22:00:00.00000060    28.760988\n",
      "9885  2013-09-25 22:00:00.000000213    14.406194\n",
      "9886  2013-09-25 22:00:00.000000150    19.227438\n",
      "9887   2013-09-25 22:00:00.00000010     9.196151\n",
      "9888  2013-09-25 22:00:00.000000146     5.625739\n",
      "9889   2013-09-25 22:00:00.00000041     9.636006\n",
      "9890  2013-09-25 22:00:00.000000109    10.994185\n",
      "9891  2013-09-25 22:00:00.000000210    15.529410\n",
      "9892  2013-09-25 22:00:00.000000151    10.570165\n",
      "9893  2013-09-25 22:00:00.000000190    14.692330\n",
      "9894  2013-09-25 22:00:00.000000153    10.882781\n",
      "9895  2013-09-25 22:00:00.000000241    22.546314\n",
      "9896  2013-09-25 22:00:00.000000127    10.699520\n",
      "9897    2015-02-20 11:08:29.0000001    17.599869\n",
      "9898    2015-01-12 15:36:37.0000002     6.959829\n",
      "9899    2015-06-07 00:38:14.0000002    20.182306\n",
      "9900    2015-04-12 21:56:22.0000005     9.723223\n",
      "9901    2015-04-10 11:56:54.0000004    10.006954\n",
      "9902    2015-06-25 01:01:46.0000002    13.757163\n",
      "9903    2015-05-29 10:02:42.0000001    11.042626\n",
      "9904    2015-06-30 20:03:50.0000002    37.043697\n",
      "9905    2015-02-27 19:36:02.0000006    25.799994\n",
      "9906    2015-06-15 01:00:06.0000002     6.700007\n",
      "9907    2015-02-03 09:00:58.0000001    33.031563\n",
      "9908    2015-05-19 13:58:11.0000001     9.405787\n",
      "9909    2015-05-10 12:37:51.0000002    10.225909\n",
      "9910    2015-01-12 17:05:51.0000001    11.532499\n",
      "9911    2015-04-19 20:44:15.0000001    41.925987\n",
      "9912    2015-01-31 01:05:19.0000005    20.139835\n",
      "9913    2015-01-18 14:06:23.0000006     9.298677\n",
      "\n",
      "[9914 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "kaggle_submit('dl_model.json', keras=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda\n",
    "# p42xlarge\n",
    "# ec2 instance pricing\n",
    "# make sure you have gpu and optimization\n",
    "# save as pickle file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
