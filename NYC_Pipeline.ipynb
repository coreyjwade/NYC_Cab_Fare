{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 6000000\n",
    "NODES = [100,100,50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import model_from_json\n",
    "from sklearn.externals import joblib\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_dataFrame(file_name, subset=True, nrows=ROWS):\n",
    "    if subset:\n",
    "        df = pd.read_csv(file_name, nrows=nrows, parse_dates=['pickup_datetime'])\n",
    "    else:\n",
    "        df = pd.read_csv(file_name, parse_dates=['pickup_datetime'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xtest_ytest(df, split=False):\n",
    "    y_test = df['key']\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    X_test = df.drop('key', axis=1)\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    x = len(df)\n",
    "    print('Length of df:', x)\n",
    "    df = df.dropna(axis=0, subset=['dropoff_latitude'])\n",
    "    df = df.drop('key', axis=1)\n",
    "    y = len(df)\n",
    "    print('NaN dropped:', x-y)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_US(df):\n",
    "    x = len(df)\n",
    "    # Choose cab rides whose pickup and dropoff are the US Mainland\n",
    "    # Declare constants\n",
    "    latmin = 5.496100\n",
    "    latmax = 71.538800\n",
    "    longmin = -124.482003\n",
    "    longmax = -66.885417\n",
    "\n",
    "    # Create dataframe with correct coordinates\n",
    "    df = df[((((df['pickup_longitude']<=longmax) & (df['pickup_longitude']>=longmin)) & ((df['pickup_latitude']<=latmax) & (df['pickup_latitude']>=latmin)))) & ((((df['dropoff_longitude']<=longmax) & (df['dropoff_longitude']>=longmin)) & ((df['dropoff_latitude']<=latmax) & (df['dropoff_latitude']>=latmin))))]\n",
    "    \n",
    "    print('US Mainland Only dropped:', x-len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_NYC(df):\n",
    "    x = len(df)\n",
    "    # Find cab rides whose pickup or dropoff are within NYC boundaries\n",
    "    # Declare constants\n",
    "    latmin = 40.477399\n",
    "    latmax = 40.917577\n",
    "    longmin = -74.259090\n",
    "    longmax = -73.700272\n",
    "\n",
    "    # Create dataframe with correct coordinates\n",
    "    df = df[((((df['pickup_longitude']<=longmax) & (df['pickup_longitude']>=longmin)) & ((df['pickup_latitude']<=latmax) & (df['pickup_latitude']>=latmin)))) | ((((df['dropoff_longitude']<=longmax) % (df['dropoff_longitude']>=longmin)) & ((df['dropoff_latitude']<=latmax) & (df['dropoff_latitude']>=latmin))))]\n",
    "    \n",
    "    print('NYC Taxis Only dropped:', x-len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_Riders(df, num=6):\n",
    "    x = len(df)\n",
    "    # Only choose cabs between 1 and num riders\n",
    "    df = df[(df['passenger_count'] <= num) & (df['passenger_count'] > 0)]\n",
    "    print('Max Passengers 6 dropped:',  x-len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance(df):\n",
    "\n",
    "    # Define coordinates (x,y)\n",
    "    x1 = df['pickup_latitude']\n",
    "    y1 = df['pickup_longitude']\n",
    "    x2 = df['dropoff_latitude']\n",
    "    y2 = df['dropoff_longitude']\n",
    "\n",
    "    # Create Euclidean Distrance column\n",
    "    df['euclidean_distance'] = np.sqrt((y2-y1)**2 + (x2-x1)**2)\n",
    "\n",
    "    # Create Taxicab Distance column\n",
    "    df['taxicab_distance'] = np.abs(y2-y1) + np.abs(x2-x1)\n",
    "\n",
    "    # Convert to miles\n",
    "    df['euclidean_distance'] = df['euclidean_distance'] * 69\n",
    "    df['taxicab_distance'] = df['taxicab_distance'] * 69\n",
    "    \n",
    "    print('Distance Columns added...')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_Fare(df):\n",
    "    # Eliminate unrealistic plots\n",
    "    df = df[df['fare_amount'] >= (df['euclidean_distance'] * 2 + 2.5)]\n",
    "    print('Min fares dropped:', len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_Fare(df):\n",
    "    df = df[(df['fare_amount'] <= (df['taxicab_distance'] * 48 + 16)) | (df['fare_amount'] <= 56)]\n",
    "    print('Max fares dropped:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_distance(df):\n",
    "    # Elminate fares that traveled no distance\n",
    "    df = df[df['euclidean_distance']>0]\n",
    "    print('No distance dropped:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_cap(df, cap=75):\n",
    "    df = df[df['euclidean_distance'] < cap]\n",
    "    print('Distance cap dropped:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_elimination(df):\n",
    "    df = clean_data(df)\n",
    "    df = lat_lon_US(df)\n",
    "    df = lat_lon_NYC(df)\n",
    "    df = max_Riders(df)\n",
    "    df = add_distance(df)\n",
    "    #df = min_Fare(df)\n",
    "    #df = max_Fare(df)\n",
    "    #df = no_distance(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_train, y_train Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_y(df, split=False):\n",
    "    X = df.drop('fare_amount', axis=1)\n",
    "    y = df['fare_amount'].copy()\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garbage Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of accumulated garbage\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Time_units(df):\n",
    "    \n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['minute'] = df['pickup_datetime'].dt.minute\n",
    "    df['second'] = df['pickup_datetime'].dt.second\n",
    "    df['dayofweek'] = df['pickup_datetime'].dt.dayofweek\n",
    "    \n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "    dr = pd.date_range(start='2009-01-01', end='2015-12-31')\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "    df['holiday'] = df['pickup_datetime'].dt.date.astype('datetime64').isin(holidays)\n",
    "    \n",
    "    df = df.drop('pickup_datetime', axis=1)\n",
    "\n",
    "    df['total_seconds'] = 3600 * df['hour'] + 60 * df['minute'] + df['second']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Time_columns(df):\n",
    "    \n",
    "    def morning_rush(row):\n",
    "        if ((row['hour'] in [6,7,8,9]) & (row['dayofweek'] in [0,1,2,3,4])) & (not row['holiday']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['morning_rush'] = df.apply(morning_rush, axis=1)\n",
    "\n",
    "    def night_charge(row):\n",
    "        if row['hour'] in [20,21,22,23,24,1,2,3,4,5,6]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['night_charge'] = df.apply(night_charge, axis=1)\n",
    "\n",
    "    def weekday_surcharge(row):\n",
    "        if ((row['hour'] in [16,17,18,19,20]) & (row['dayofweek'] in [0,1,2,3,4])) & (not row['holiday']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['weekday_surcharge'] = df.apply(weekday_surcharge, axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Time(df):\n",
    "    df = add_Time_units(df)\n",
    "    df = add_Time_columns(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define line from two points and a provided column\n",
    "def two_points_line(a, b, column):\n",
    "        \n",
    "    # Case when y-values are the same\n",
    "    if b[1]==a[1]:\n",
    "        \n",
    "        # Slope defaults to 0\n",
    "        slope = 0\n",
    "        \n",
    "    # Case when x-values are the same\n",
    "    elif b[0]==a[0]:\n",
    "        \n",
    "        # Case when max value is less than 999999999\n",
    "        if column.max() < 999999999:\n",
    "            \n",
    "            # Add 999999999 to max value\n",
    "            slope = column.max() + 999999999\n",
    "        \n",
    "        # All other cases\n",
    "        else:\n",
    "            \n",
    "            # Multiply max value by itself (greater than 999999999)\n",
    "            slope = column.max() * column.max()\n",
    "    \n",
    "    # When x-values and y-values are not 0\n",
    "    else:\n",
    "        \n",
    "        # Use standard slope formula\n",
    "        slope = (b[1] - a[1])/(b[0]-a[0])\n",
    "    \n",
    "    \n",
    "    # Equation for y-intercept (solving y=mx+b for b)\n",
    "    y_int = a[1] - slope * a[0]\n",
    "    \n",
    "    # Return slope and y-intercept\n",
    "    return slope, y_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_cols(df):\n",
    "    \n",
    "    upper_right = (-73.929224, 40.804328)\n",
    "    bottom_right = (-73.980036, 40.710706)\n",
    "    bottom_left = (-74.054880, 40.681292)\n",
    "    upper_left = (-73.966303, 40.830050)\n",
    "\n",
    "    m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "    m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "    m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "    m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "    def manhattan_pickup(row):\n",
    "        if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "        (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['pickup_latitude'] >= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "        (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    df['manhattan_pickup'] = df.apply(manhattan_pickup, axis=1)\n",
    "    \n",
    "    \n",
    "    def manhattan_dropoff(row):\n",
    "        if (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "        (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "        (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    df['manhattan_dropoff'] = df.apply(manhattan_dropoff, axis=1)\n",
    "    \n",
    "    \n",
    "    def manhattan(row):\n",
    "        if (row['manhattan_pickup']) & (row['manhattan_dropoff']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    df['manhattan'] = df.apply(manhattan, axis=1)\n",
    "    \n",
    "    \n",
    "    def manhattan_one_way(row):\n",
    "        if (not row['manhattan']) & (row['manhattan_pickup']) | (row['manhattan_dropoff']):\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "    df['manhattan_one_way'] = df.apply(manhattan_one_way, axis=1)\n",
    "     \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def manhattan_cols(df):\n",
    "    \n",
    "#     upper_right = (-73.929224, 40.804328)\n",
    "#     bottom_right = (-73.980036, 40.710706)\n",
    "#     bottom_left = (-74.054880, 40.681292)\n",
    "#     upper_left = (-73.966303, 40.830050)\n",
    "\n",
    "#     m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "#     m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "#     m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "#     m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "#     def manhattan(row):\n",
    "#         if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "#         (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "#         ((row['pickup_latitude'] >= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "#         (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))) & (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "#         (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "#         ((row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "#         (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "#             return 1\n",
    "#         else:\n",
    "#             return 0\n",
    "        \n",
    "#     df['manhattan'] = df.apply(manhattan, axis=1)\n",
    "        \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newark_cols(df):\n",
    "    \n",
    "    upper_right = (-74.107867, 40.718282)\n",
    "    bottom_right = (-74.143665, 40.654673)\n",
    "    bottom_left = (-74.250524, 40.698436)\n",
    "    upper_left = (-74.171983, 40.792347)\n",
    "\n",
    "    m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "    m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "    m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "    m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "    def newark(row):\n",
    "        if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "        (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['pickup_latitude'] >= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "        (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))) | (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "        (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "        (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    df['newark'] = df.apply(newark, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jkf_cols(df):\n",
    "    \n",
    "    upper_right = (-73.789700, 40.663781)\n",
    "    bottom_right = (-73.762112, 40.633567)\n",
    "    bottom_left = (-73.818920, 40.642250)\n",
    "    upper_left = (-73.804656, 40.664858)\n",
    "\n",
    "    m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "    m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "    m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "    m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "    def jfk(row):\n",
    "        if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "        (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['pickup_latitude'] <= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "        (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))) | (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "        (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "        (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    df['jfk'] = df.apply(jfk, axis=1)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_locations(df):\n",
    "    df = manhattan_cols(df)\n",
    "    df = jkf_cols(df)\n",
    "    df = newark_cols(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cols(df):\n",
    "    df = add_Time(df)\n",
    "    df = add_locations(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_predictor_cols(df, no_dist=False): \n",
    "    if no_dist:\n",
    "        cols=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'year', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'total_seconds', 'morning_rush', 'night_charge', 'weekday_surcharge', 'manhattan', 'manhattan_one_way', 'jfk', 'newark', 'passenger_count', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "    else: \n",
    "        cols=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'year', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'total_seconds', 'morning_rush', 'night_charge', 'weekday_surcharge', 'manhattan', 'manhattan_one_way', 'jfk', 'newark', 'passenger_count','euclidean_distance', 'taxicab_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "    X = df[cols]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_cols(X):\n",
    "    X = one_Hot_Encoder(X, X['month'])\n",
    "    del X['month']\n",
    "    X = one_Hot_Encoder(X, X['dayofweek'], month=False)\n",
    "    del X['dayofweek']\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_Hot_Encoder(X, col, month=True): \n",
    "    encoder = OneHotEncoder()\n",
    "    hot_array = encoder.fit_transform(np.array(col).reshape(-1,1)).toarray()\n",
    "    hot_df = pd.DataFrame(hot_array)\n",
    "    if month:\n",
    "        hot_df.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    else:\n",
    "        hot_df.columns = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "    new_df = X.join(hot_df)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_split(X_train, y_train):\n",
    "            \n",
    "    y = y_train.median()\n",
    "    mse = np.sum((y_train-y)**2)\n",
    "    score = mse/len(y_train)\n",
    "    rmse = np.sqrt(score)\n",
    "    print('Lin reg train rmse:', rmse)\n",
    "    print('Lin reg train mean:', rmse.mean())\n",
    "    print('Lin reg train std:', rmse.std())\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X_train, y_train, distance_none=False, distance_high=False):\n",
    "        \n",
    "    print('Length of X:', len(X_train))\n",
    "    lr_model = LinearRegression(fit_intercept=False)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    scores = cross_val_score(lr_model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse = np.sqrt(-scores)\n",
    "    print('Lin reg train rmse:', rmse)\n",
    "    print('Lin reg train mean:', rmse.mean())\n",
    "    print('Lin reg train std:', rmse.std())\n",
    "    \n",
    "    if distance_none:\n",
    "        joblib.dump(lr_model, 'lr_distance_none_model.pkl')\n",
    "        print('Linear Regression model saved as \"lr_distance_none_model.pkl\"')\n",
    "    elif distance_high:\n",
    "        joblib.dump(lr_model, 'lr_distance_high_model.pkl')\n",
    "        print('Linear Regression model saved as \"lr_distance_high_model.pkl\"')\n",
    "    else:\n",
    "        joblib.dump(lr_model, 'lr_model.pkl') \n",
    "        print('Linear Regression model saved as \"lr_model.pkl\"')\n",
    "\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(X_train, y_train, distance_none=False, distance_high=False):\n",
    "        \n",
    "    print('Length of X:', len(X_train))\n",
    "    ri_model = Ridge()\n",
    "    ri_model.fit(X_train, y_train)\n",
    "    scores = cross_val_score(ri_model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse = np.sqrt(-scores)\n",
    "    print('Lin reg train rmse:', rmse)\n",
    "    print('Lin reg train mean:', rmse.mean())\n",
    "    print('Lin reg train std:', rmse.std())\n",
    "    \n",
    "    if distance_none:\n",
    "        joblib.dump(ri_model, 'ri_distance_none_model.pkl')\n",
    "        print('Linear Regression model saved as \"ri_distance_none_model.pkl\"')\n",
    "    elif distance_high:\n",
    "        joblib.dump(ri_model, 'ri_distance_high_model.pkl')\n",
    "        print('Linear Regression model saved as \"ri_distance_high_model.pkl\"')\n",
    "    else:\n",
    "        joblib.dump(ri_model, 'ri_model.pkl') \n",
    "        print('Linear Regression model saved as \"ri_model.pkl\"')\n",
    "\n",
    "    return ri_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_random_forest_tuner(X_train, y_train):\n",
    "        \n",
    "    param_grid = [\n",
    "        {'n_estimators': [75, 100, 250, 500, 750, 1000, 15000], 'max_features': [5, 10, 15, 20, 25]}, \n",
    "    ]\n",
    "    \n",
    "    forest_reg = RandomForestRegressor()\n",
    "    \n",
    "    forest_reg_tuned = RandomSearchCV(forest_reg, param_grid, n_iter=6, cv=3, \n",
    "                                    scoring='neg_mean_squared_error')\n",
    "    \n",
    "    forest_reg_tuned.fit(X,y)\n",
    "    \n",
    "    # Print the tuned parameters and score\n",
    "    print(\"Tuned Random Forest Parameters: {}\".format(forest_reg_tuned.best_params_))\n",
    "    \n",
    "    scores = cross_val_score(forest_reg_tuned, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "    \n",
    "    display_scores('Random Forest', scores)\n",
    "    \n",
    "    return forest_reg_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_tuner(X_train, y_train):\n",
    "        \n",
    "    param_grid = [\n",
    "        {'n_estimators': [100, 500, 1000], 'max_features': [10]}, \n",
    "    ]\n",
    "    \n",
    "    forest_reg = RandomForestRegressor()\n",
    "    \n",
    "    forest_reg_tuned = GridSearchCV(forest_reg, param_grid, cv=3, \n",
    "                                    scoring='neg_mean_squared_error')\n",
    "    \n",
    "    forest_reg_tuned.fit(X,y)\n",
    "    \n",
    "    # Print the tuned parameters and score\n",
    "    print(\"Tuned Random Forest Parameters: {}\".format(forest_reg_tuned.best_params_))\n",
    "    \n",
    "    scores = cross_val_score(forest_reg_tuned, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "    \n",
    "    display_scores('Random Forest', scores)\n",
    "    \n",
    "    return forest_reg_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(title, scores):\n",
    "    rmse = np.sqrt(-scores)\n",
    "    print(title, ' rmse scores:', rmse)\n",
    "    print(title, ' mean score:', rmse.mean())\n",
    "    print(title, ' std:', rmse.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, distance_none=False, distance_high=False):\n",
    "    \n",
    "    rf_model = RandomForestRegressor(max_features=10, n_estimators=50)\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    scores = cross_val_score(rf_model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    display_scores('Random Forest', scores)\n",
    "    \n",
    "    if distance_none:\n",
    "        joblib.dump(rf_model, 'rf_distance_none_model.pkl')\n",
    "        print('Linear Regression model saved as \"rf_distance_none_model.pkl\"')\n",
    "    elif distance_high:\n",
    "        joblib.dump(rf_model, 'rf_distance_high_model.pkl')\n",
    "        print('Linear Regression model saved as \"rf_distance_high_model.pkl\"')\n",
    "    else:\n",
    "        joblib.dump(rf_model, 'rf_model.pkl') \n",
    "        print('Linear Regression model saved as \"rf_model.pkl\"')\n",
    "        \n",
    "    return rf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_regression_test requires \"from sklearn.model_selection import train_test_split\"\n",
    "def deep_learning(X_train, y_train, nodes=NODES, batch_size=32, activation='relu', optimizer='adam', loss='mean_squared_error', keras_distance_high=False, keras_distance_none=False):\n",
    "        \n",
    "    X, X_check, y, y_check = train_test_split(X_train, y_train, test_size=0.05)\n",
    "    \n",
    "    # Save the number of columns in predictors: n_cols\n",
    "    n_cols = X.shape[1]\n",
    "\n",
    "    # Set up the model: model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add the first layer\n",
    "    model.add(Dense(nodes[0], activation=activation, input_shape=(n_cols,)))\n",
    "    \n",
    "    # Add addition layers\n",
    "    for i in range(len(nodes)-1):\n",
    "        model.add(Dense(nodes[i+1], activation=activation, kernel_constraint=maxnorm(3)))\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    # Define early_stopping_monitor\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X, y, validation_split=0.05, epochs=1000, batch_size=batch_size, callbacks=[early_stopping_monitor])\n",
    "\n",
    "    # Get score for predictions\n",
    "    score = model.evaluate(X_check, y_check)\n",
    "    \n",
    "    # Get root mean squared error\n",
    "    rmse = np.sqrt(score)\n",
    "    \n",
    "    # Return root mean squared error\n",
    "    print(rmse)\n",
    "    \n",
    "    save_keras_model(model, keras_distance_high=keras_distance_high, keras_distance_none=keras_distance_none)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(X):\n",
    "    X = X.reset_index(drop=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_frame_split(df):\n",
    "    \n",
    "    df_distance_none = df[df['euclidean_distance']==0]\n",
    "    print('New dataframe \"df_distance_none\" created with length:', len(df_distance_none))\n",
    "    \n",
    "    df_distance_high = df[df['euclidean_distance']>30]\n",
    "    print('New dataframe \"df_distance_high\" created with length:', len(df_distance_high))\n",
    "\n",
    "    df = df[df['euclidean_distance']>0]\n",
    "    df = df[df['euclidean_distance']<=30]\n",
    "\n",
    "    print('New length of original dataframe:', len(df))\n",
    "    return df, df_distance_none, df_distance_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_pipeline(df, no_dist=False):\n",
    "    df = reset_index(df)\n",
    "    df = add_cols(df)\n",
    "    df = one_hot_cols(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_pipeline(X, no_dist=False):\n",
    "    X = choose_predictor_cols(X, no_dist=no_dist)\n",
    "    X = min_max_scaler(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline(test_set=False, max_scaler=True):\n",
    "    df = file_to_dataFrame('test.csv')\n",
    "    print('Length of test_df:)', len(df))\n",
    "    df = add_distance(df)\n",
    "    df = df_pipeline(df)\n",
    "    df, df_distance_none, df_distance_high = data_frame_split(df)\n",
    "    \n",
    "    X_test, y_test = make_Xtest_ytest(df)\n",
    "    X_test_distance_none, y_test_distance_none = make_Xtest_ytest(df_distance_none)\n",
    "    X_test_distance_high, y_test_distance_high = make_Xtest_ytest(df_distance_high)\n",
    "    \n",
    "    X_test = X_pipeline(df)\n",
    "    X_test_distance_none = X_pipeline(X_test_distance_none, no_dist=True)\n",
    "    X_test_distance_high = X_pipeline(X_test_distance_high)\n",
    "    \n",
    "    return X_test, y_test, X_test_distance_none, y_test_distance_none, X_test_distance_high, y_test_distance_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "    \n",
    "    df = file_to_dataFrame('train.csv')\n",
    "    df = row_elimination(df)\n",
    "    df = df_pipeline(df)\n",
    "    df, df_distance_none, df_distance_high = data_frame_split(df)\n",
    "    \n",
    "    X, y = make_X_y(df)\n",
    "    X_distance_none, y_distance_none = make_X_y(df_distance_none)\n",
    "    X_distance_high, y_distance_high = make_X_y(df_distance_high)\n",
    "    \n",
    "    X = X_pipeline(X)\n",
    "    X_distance_none = X_pipeline(X_distance_none, no_dist=True)\n",
    "    X_distance_high = X_pipeline(X_distance_high)\n",
    "    \n",
    "    return X, y, X_distance_none, y_distance_none, X_distance_high, y_distance_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keras_model(model, keras_distance_none=False, keras_distance_high=False):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    if keras_distance_none:\n",
    "        with open(\"dl_distance_none_model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"dl_distance_none_model.h5\")\n",
    "        print(\"Saved deep learning model as 'dl_distance_none_model.json'\")\n",
    "    \n",
    "    elif keras_distance_high:\n",
    "        with open(\"dl_distance_high_model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"dl_distance_high_model.h5\")\n",
    "        print(\"Saved deep learning model as 'dl_distance_high_model.json'\")\n",
    "    \n",
    "    else:\n",
    "        with open(\"dl_model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"dl_model.h5\")\n",
    "        print(\"Saved deep learning model as 'dl_model.json'\")\n",
    "    return model\n",
    "  \n",
    "def open_keras_model(file, keras_distance_none=False, keras_distance_high=False):\n",
    "    # load json and create model\n",
    "    json_file = open(file, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    if keras_distance_none:\n",
    "        loaded_model.load_weights(\"dl_distance_none_model.h5\")\n",
    "    elif keras_distance_high:\n",
    "        loaded_model.load_weights(\"dl_distance_high_model.h5\")\n",
    "    else:\n",
    "        loaded_model.load_weights(\"dl_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_model(saved_model, keras=False, keras_distance_none=False, keras_distance_high=False):\n",
    "    if keras:\n",
    "        model = open_keras_model(saved_model)\n",
    "    elif keras_distance_none:\n",
    "        model = open_keras_model(saved_model, keras_distance_none=keras_distance_none)\n",
    "    elif keras_distance_high:\n",
    "        model = open_keras_model(saved_model, keras_distance_high=keras_distance_high)\n",
    "    else:\n",
    "        model = joblib.load(saved_model)\n",
    "    return model\n",
    "\n",
    "def kaggle_submit(y_test, saved_model, saved_model_distance_none, saved_model_distance_high, keras=False, keras_distance_none=False, keras_distance_high=False):\n",
    "    saved_model = open_model(saved_model, keras=keras)\n",
    "    saved_model_distance_none = open_model(saved_model_distance_none, keras_distance_none=keras_distance_none)\n",
    "    saved_model_distance_high = open_model(saved_model_distance_high, keras_distance_high=keras_distance_high)\n",
    "        \n",
    "    y_test['fare_amount'] = saved_model.predict(X_test)\n",
    "    y_test_distance_none['fare_amount'] = saved_model_distance_none.predict(X_test_distance_none)\n",
    "    y_test_distance_high['fare_amount'] = saved_model_distance_high.predict(X_test_distance_high)\n",
    "    \n",
    "    y_test = pd.concat([y_test,y_test_distance_none, y_test_distance_high])\n",
    "    \n",
    "    y_test.to_csv('my_submission.csv', index=False)\n",
    "    #print(y_test)\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of df: 150000\n",
      "NaN dropped: 1\n",
      "US Mainland Only dropped: 3088\n",
      "NYC Taxis Only dropped: 179\n",
      "Max Passengers 6 dropped: 540\n",
      "Distance Columns added...\n",
      "New dataframe \"df_distance_none\" created with length: 1524\n",
      "New dataframe \"df_distance_high\" created with length: 23\n",
      "New length of original dataframe: 144645\n"
     ]
    }
   ],
   "source": [
    "X, y, X_distance_none, y_distance_none, X_distance_high, y_distance_high = pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_df:) 9914\n",
      "Distance Columns added...\n",
      "New dataframe \"df_distance_none\" created with length: 85\n",
      "New dataframe \"df_distance_high\" created with length: 3\n",
      "New length of original dataframe: 9826\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test, X_test_distance_none, y_test_distance_none, X_test_distance_high, y_test_distance_high = test_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 23\n",
      "Lin reg train rmse: [ 50.09166181  99.24755498 130.73293799 116.24477776 102.93735937]\n",
      "Lin reg train mean: 99.85085838115526\n",
      "Lin reg train std: 27.233551147597566\n",
      "Linear Regression model saved as \"lr_distance_high_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression(X_distance_high, y_distance_high, distance_high=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 1524\n",
      "Lin reg train rmse: [10.75368784 14.23409137 12.70426881 32.36891942 11.05286343]\n",
      "Lin reg train mean: 16.22276617292958\n",
      "Lin reg train std: 8.169108004208924\n",
      "Linear Regression model saved as \"lr_distance_none_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression(X_distance_none, y_distance_none, distance_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 144645\n",
      "Lin reg train rmse: [4.00703791 4.2312971  4.23811799 3.96156074 4.20959174]\n",
      "Lin reg train mean: 4.1295210972457825\n",
      "Lin reg train std: 0.11981299089025096\n",
      "Linear Regression model saved as \"lr_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 144645\n",
      "Lin reg train rmse: [4.00935679 4.23126438 4.2410177  3.9619879  4.21230153]\n",
      "Lin reg train mean: 4.1311856599802494\n",
      "Lin reg train std: 0.12010725998495231\n",
      "Linear Regression model saved as \"ri_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 1524\n",
      "Lin reg train rmse: [10.78924625 14.25718165 12.6087368  32.21592414 11.03182682]\n",
      "Lin reg train mean: 16.180583132890966\n",
      "Lin reg train std: 8.113765164910333\n",
      "Linear Regression model saved as \"ri_distance_none_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge(X_distance_none, y_distance_none, distance_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 23\n",
      "Lin reg train rmse: [ 20.42250476  55.2327055   59.77037005 111.44322998  66.35027968]\n",
      "Lin reg train mean: 62.64381799387196\n",
      "Lin reg train std: 29.123670961938718\n",
      "Linear Regression model saved as \"ri_distance_high_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge(X_distance_high, y_distance_high, distance_high=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-27 13:08:24.0000002</td>\n",
       "      <td>11.831175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-27 13:08:24.0000003</td>\n",
       "      <td>12.076457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-08 11:53:44.0000002</td>\n",
       "      <td>7.088711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-01 21:12:12.0000002</td>\n",
       "      <td>9.404295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-01 21:12:12.0000003</td>\n",
       "      <td>15.789187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012-12-01 21:12:12.0000005</td>\n",
       "      <td>11.876712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2011-10-06 12:10:20.0000001</td>\n",
       "      <td>8.425235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2011-10-06 12:10:20.0000003</td>\n",
       "      <td>55.276801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2011-10-06 12:10:20.0000002</td>\n",
       "      <td>13.497633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2014-02-18 15:22:20.0000002</td>\n",
       "      <td>9.469924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2014-02-18 15:22:20.0000003</td>\n",
       "      <td>11.643593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2014-02-18 15:22:20.0000001</td>\n",
       "      <td>17.092475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2010-03-29 20:20:32.0000002</td>\n",
       "      <td>5.366788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2010-03-29 20:20:32.0000001</td>\n",
       "      <td>8.067687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2011-10-06 03:59:12.0000002</td>\n",
       "      <td>9.550834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2011-10-06 03:59:12.0000001</td>\n",
       "      <td>14.327304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2012-07-15 16:45:04.0000006</td>\n",
       "      <td>6.729548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012-07-15 16:45:04.0000002</td>\n",
       "      <td>11.038239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2012-07-15 16:45:04.0000003</td>\n",
       "      <td>7.261642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2012-07-15 16:45:04.0000004</td>\n",
       "      <td>6.894919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2014-10-29 02:09:56.0000001</td>\n",
       "      <td>11.744842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2014-06-14 13:39:00.00000010</td>\n",
       "      <td>11.492544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2014-06-14 13:39:00.00000060</td>\n",
       "      <td>9.617189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2014-06-14 13:39:00.00000087</td>\n",
       "      <td>10.684886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2014-06-14 13:39:00.00000050</td>\n",
       "      <td>18.183872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2014-06-14 13:39:00.0000003</td>\n",
       "      <td>9.617329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2014-06-14 13:39:00.000000158</td>\n",
       "      <td>38.306692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2014-06-14 13:39:00.00000015</td>\n",
       "      <td>24.269827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2014-06-14 13:39:00.00000073</td>\n",
       "      <td>9.128997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2014-06-14 13:39:00.00000077</td>\n",
       "      <td>16.668862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>2010-08-14 02:13:00.0000003</td>\n",
       "      <td>28.948200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7198</th>\n",
       "      <td>2010-09-29 16:43:00.000000172</td>\n",
       "      <td>22.585695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7322</th>\n",
       "      <td>2011-10-04 09:37:00.00000077</td>\n",
       "      <td>15.924799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7526</th>\n",
       "      <td>2011-03-06 21:01:00.00000081</td>\n",
       "      <td>39.893592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>2012-01-26 07:33:00.000000136</td>\n",
       "      <td>21.335400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>2012-01-26 07:33:00.00000023</td>\n",
       "      <td>21.933786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7637</th>\n",
       "      <td>2010-12-09 07:29:00.00000083</td>\n",
       "      <td>19.979684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7839</th>\n",
       "      <td>2012-12-15 06:35:45.0000001</td>\n",
       "      <td>27.161724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7874</th>\n",
       "      <td>2009-06-02 08:34:33.0000001</td>\n",
       "      <td>19.340884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>2013-01-19 20:19:45.0000002</td>\n",
       "      <td>19.037468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8159</th>\n",
       "      <td>2011-07-20 08:05:02.0000001</td>\n",
       "      <td>27.446033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381</th>\n",
       "      <td>2009-07-30 15:49:15.0000002</td>\n",
       "      <td>-1.785270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8424</th>\n",
       "      <td>2009-06-10 16:55:00.000000155</td>\n",
       "      <td>17.734243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8426</th>\n",
       "      <td>2011-06-24 12:03:00.000000131</td>\n",
       "      <td>16.132566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8517</th>\n",
       "      <td>2011-03-11 01:44:00.00000067</td>\n",
       "      <td>17.941620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596</th>\n",
       "      <td>2010-09-20 16:48:00.00000021</td>\n",
       "      <td>23.476067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8632</th>\n",
       "      <td>2009-10-29 10:48:40.0000003</td>\n",
       "      <td>17.685062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8835</th>\n",
       "      <td>2012-12-01 21:12:12.0000001</td>\n",
       "      <td>21.019259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9006</th>\n",
       "      <td>2013-07-02 22:27:14.0000001</td>\n",
       "      <td>23.440672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221</th>\n",
       "      <td>2010-08-27 18:45:00.000000215</td>\n",
       "      <td>14.778430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9383</th>\n",
       "      <td>2011-03-11 01:44:00.00000064</td>\n",
       "      <td>15.520683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9421</th>\n",
       "      <td>2011-10-04 09:37:00.00000088</td>\n",
       "      <td>15.157274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9478</th>\n",
       "      <td>2011-12-13 22:00:00.00000044</td>\n",
       "      <td>22.540433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>2014-07-21 18:19:00.00000025</td>\n",
       "      <td>14.544848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9563</th>\n",
       "      <td>2011-03-06 21:01:00.00000018</td>\n",
       "      <td>17.065636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9830</th>\n",
       "      <td>2014-07-21 18:19:00.00000065</td>\n",
       "      <td>13.796918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9888</th>\n",
       "      <td>2013-09-25 22:00:00.000000146</td>\n",
       "      <td>18.591142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>2010-06-11 13:37:21.0000004</td>\n",
       "      <td>-46.671294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5887</th>\n",
       "      <td>2010-07-04 16:44:11.0000002</td>\n",
       "      <td>-11.278949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8529</th>\n",
       "      <td>2009-11-25 19:32:52.0000001</td>\n",
       "      <td>66.820940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9914 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                key  fare_amount\n",
       "0       2015-01-27 13:08:24.0000002    11.831175\n",
       "1       2015-01-27 13:08:24.0000003    12.076457\n",
       "2       2011-10-08 11:53:44.0000002     7.088711\n",
       "3       2012-12-01 21:12:12.0000002     9.404295\n",
       "4       2012-12-01 21:12:12.0000003    15.789187\n",
       "5       2012-12-01 21:12:12.0000005    11.876712\n",
       "6       2011-10-06 12:10:20.0000001     8.425235\n",
       "7       2011-10-06 12:10:20.0000003    55.276801\n",
       "8       2011-10-06 12:10:20.0000002    13.497633\n",
       "9       2014-02-18 15:22:20.0000002     9.469924\n",
       "10      2014-02-18 15:22:20.0000003    11.643593\n",
       "11      2014-02-18 15:22:20.0000001    17.092475\n",
       "12      2010-03-29 20:20:32.0000002     5.366788\n",
       "13      2010-03-29 20:20:32.0000001     8.067687\n",
       "14      2011-10-06 03:59:12.0000002     9.550834\n",
       "15      2011-10-06 03:59:12.0000001    14.327304\n",
       "16      2012-07-15 16:45:04.0000006     6.729548\n",
       "17      2012-07-15 16:45:04.0000002    11.038239\n",
       "18      2012-07-15 16:45:04.0000003     7.261642\n",
       "19      2012-07-15 16:45:04.0000004     6.894919\n",
       "20      2014-10-29 02:09:56.0000001    11.744842\n",
       "21     2014-06-14 13:39:00.00000010    11.492544\n",
       "22     2014-06-14 13:39:00.00000060     9.617189\n",
       "23     2014-06-14 13:39:00.00000087    10.684886\n",
       "24     2014-06-14 13:39:00.00000050    18.183872\n",
       "25      2014-06-14 13:39:00.0000003     9.617329\n",
       "26    2014-06-14 13:39:00.000000158    38.306692\n",
       "27     2014-06-14 13:39:00.00000015    24.269827\n",
       "28     2014-06-14 13:39:00.00000073     9.128997\n",
       "29     2014-06-14 13:39:00.00000077    16.668862\n",
       "...                             ...          ...\n",
       "7163    2010-08-14 02:13:00.0000003    28.948200\n",
       "7198  2010-09-29 16:43:00.000000172    22.585695\n",
       "7322   2011-10-04 09:37:00.00000077    15.924799\n",
       "7526   2011-03-06 21:01:00.00000081    39.893592\n",
       "7607  2012-01-26 07:33:00.000000136    21.335400\n",
       "7610   2012-01-26 07:33:00.00000023    21.933786\n",
       "7637   2010-12-09 07:29:00.00000083    19.979684\n",
       "7839    2012-12-15 06:35:45.0000001    27.161724\n",
       "7874    2009-06-02 08:34:33.0000001    19.340884\n",
       "7994    2013-01-19 20:19:45.0000002    19.037468\n",
       "8159    2011-07-20 08:05:02.0000001    27.446033\n",
       "8381    2009-07-30 15:49:15.0000002    -1.785270\n",
       "8424  2009-06-10 16:55:00.000000155    17.734243\n",
       "8426  2011-06-24 12:03:00.000000131    16.132566\n",
       "8517   2011-03-11 01:44:00.00000067    17.941620\n",
       "8596   2010-09-20 16:48:00.00000021    23.476067\n",
       "8632    2009-10-29 10:48:40.0000003    17.685062\n",
       "8835    2012-12-01 21:12:12.0000001    21.019259\n",
       "9006    2013-07-02 22:27:14.0000001    23.440672\n",
       "9221  2010-08-27 18:45:00.000000215    14.778430\n",
       "9383   2011-03-11 01:44:00.00000064    15.520683\n",
       "9421   2011-10-04 09:37:00.00000088    15.157274\n",
       "9478   2011-12-13 22:00:00.00000044    22.540433\n",
       "9500   2014-07-21 18:19:00.00000025    14.544848\n",
       "9563   2011-03-06 21:01:00.00000018    17.065636\n",
       "9830   2014-07-21 18:19:00.00000065    13.796918\n",
       "9888  2013-09-25 22:00:00.000000146    18.591142\n",
       "4080    2010-06-11 13:37:21.0000004   -46.671294\n",
       "5887    2010-07-04 16:44:11.0000002   -11.278949\n",
       "8529    2009-11-25 19:32:52.0000001    66.820940\n",
       "\n",
       "[9914 rows x 2 columns]"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kaggle_submit(y_test, 'ri_model.pkl', 'ri_distance_none_model.pkl', 'lr_distance_high_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1374 samples, validate on 73 samples\n",
      "Epoch 1/100\n",
      "1374/1374 [==============================] - 1s 777us/step - loss: 425.2135 - val_loss: 347.2690\n",
      "Epoch 2/100\n",
      "1374/1374 [==============================] - 0s 62us/step - loss: 354.1414 - val_loss: 324.9997\n",
      "Epoch 3/100\n",
      "1374/1374 [==============================] - 0s 68us/step - loss: 343.0866 - val_loss: 296.1105\n",
      "Epoch 4/100\n",
      "1374/1374 [==============================] - 0s 61us/step - loss: 336.1379 - val_loss: 290.6643\n",
      "Epoch 5/100\n",
      "1374/1374 [==============================] - 0s 70us/step - loss: 330.2295 - val_loss: 288.1767\n",
      "Epoch 6/100\n",
      "1374/1374 [==============================] - 0s 63us/step - loss: 327.6949 - val_loss: 278.6469\n",
      "Epoch 7/100\n",
      "1374/1374 [==============================] - 0s 61us/step - loss: 323.7494 - val_loss: 265.6498\n",
      "Epoch 8/100\n",
      "1374/1374 [==============================] - 0s 62us/step - loss: 319.6568 - val_loss: 274.4757\n",
      "Epoch 9/100\n",
      "1374/1374 [==============================] - 0s 63us/step - loss: 315.4593 - val_loss: 275.9502\n",
      "Epoch 10/100\n",
      "1374/1374 [==============================] - 0s 60us/step - loss: 311.2804 - val_loss: 266.8658\n",
      "77/77 [==============================] - 0s 81us/step\n",
      "12.076043737298448\n",
      "Saved deep learning model as 'dl_distance_none_model.json'\n"
     ]
    }
   ],
   "source": [
    "dl_model_distance = deep_learning(X_distance_none, y_distance_none, keras_distance_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19 samples, validate on 2 samples\n",
      "Epoch 1/100\n",
      "19/19 [==============================] - 1s 56ms/step - loss: 7339.2236 - val_loss: 9722.7441\n",
      "Epoch 2/100\n",
      "19/19 [==============================] - 0s 167us/step - loss: 7327.0527 - val_loss: 9709.0420\n",
      "Epoch 3/100\n",
      "19/19 [==============================] - 0s 166us/step - loss: 7318.3398 - val_loss: 9692.9824\n",
      "Epoch 4/100\n",
      "19/19 [==============================] - 0s 167us/step - loss: 7309.8896 - val_loss: 9676.7246\n",
      "Epoch 5/100\n",
      "19/19 [==============================] - 0s 166us/step - loss: 7305.7935 - val_loss: 9659.0898\n",
      "Epoch 6/100\n",
      "19/19 [==============================] - 0s 166us/step - loss: 7293.5215 - val_loss: 9641.1289\n",
      "Epoch 7/100\n",
      "19/19 [==============================] - 0s 161us/step - loss: 7275.6826 - val_loss: 9622.4629\n",
      "Epoch 8/100\n",
      "19/19 [==============================] - 0s 150us/step - loss: 7267.0986 - val_loss: 9603.2109\n",
      "Epoch 9/100\n",
      "19/19 [==============================] - 0s 164us/step - loss: 7255.7861 - val_loss: 9582.4736\n",
      "Epoch 10/100\n",
      "19/19 [==============================] - 0s 160us/step - loss: 7255.8223 - val_loss: 9559.7070\n",
      "Epoch 11/100\n",
      "19/19 [==============================] - 0s 149us/step - loss: 7218.0928 - val_loss: 9535.2910\n",
      "Epoch 12/100\n",
      "19/19 [==============================] - 0s 151us/step - loss: 7186.5195 - val_loss: 9509.4258\n",
      "Epoch 13/100\n",
      "19/19 [==============================] - 0s 160us/step - loss: 7183.6143 - val_loss: 9481.3965\n",
      "Epoch 14/100\n",
      "19/19 [==============================] - 0s 166us/step - loss: 7158.9229 - val_loss: 9450.6982\n",
      "Epoch 15/100\n",
      "19/19 [==============================] - 0s 159us/step - loss: 7134.9014 - val_loss: 9415.9043\n",
      "Epoch 16/100\n",
      "19/19 [==============================] - 0s 160us/step - loss: 7120.1343 - val_loss: 9378.0254\n",
      "Epoch 17/100\n",
      "19/19 [==============================] - 0s 158us/step - loss: 7107.0098 - val_loss: 9336.6182\n",
      "Epoch 18/100\n",
      "19/19 [==============================] - 0s 186us/step - loss: 7075.4077 - val_loss: 9291.4775\n",
      "Epoch 19/100\n",
      "19/19 [==============================] - 0s 262us/step - loss: 7035.9614 - val_loss: 9242.5547\n",
      "Epoch 20/100\n",
      "19/19 [==============================] - 0s 228us/step - loss: 6980.3765 - val_loss: 9190.2100\n",
      "Epoch 21/100\n",
      "19/19 [==============================] - 0s 251us/step - loss: 6973.5117 - val_loss: 9133.6289\n",
      "Epoch 22/100\n",
      "19/19 [==============================] - 0s 208us/step - loss: 6907.5327 - val_loss: 9071.6240\n",
      "Epoch 23/100\n",
      "19/19 [==============================] - 0s 238us/step - loss: 6891.7705 - val_loss: 9005.6855\n",
      "Epoch 24/100\n",
      "19/19 [==============================] - 0s 255us/step - loss: 6825.9360 - val_loss: 8934.2744\n",
      "Epoch 25/100\n",
      "19/19 [==============================] - 0s 247us/step - loss: 6830.4688 - val_loss: 8857.6523\n",
      "Epoch 26/100\n",
      "19/19 [==============================] - 0s 184us/step - loss: 6764.6431 - val_loss: 8774.7734\n",
      "Epoch 27/100\n",
      "19/19 [==============================] - 0s 193us/step - loss: 6690.8296 - val_loss: 8685.3184\n",
      "Epoch 28/100\n",
      "19/19 [==============================] - 0s 225us/step - loss: 6626.8511 - val_loss: 8587.9277\n",
      "Epoch 29/100\n",
      "19/19 [==============================] - 0s 153us/step - loss: 6521.6621 - val_loss: 8483.4395\n",
      "Epoch 30/100\n",
      "19/19 [==============================] - 0s 159us/step - loss: 6486.6030 - val_loss: 8370.2041\n",
      "Epoch 31/100\n",
      "19/19 [==============================] - 0s 203us/step - loss: 6394.0361 - val_loss: 8246.9990\n",
      "Epoch 32/100\n",
      "19/19 [==============================] - 0s 158us/step - loss: 6392.7705 - val_loss: 8117.8394\n",
      "Epoch 33/100\n",
      "19/19 [==============================] - 0s 156us/step - loss: 6228.9746 - val_loss: 7980.0234\n",
      "Epoch 34/100\n",
      "19/19 [==============================] - 0s 153us/step - loss: 6124.9165 - val_loss: 7834.1660\n",
      "Epoch 35/100\n",
      "19/19 [==============================] - 0s 165us/step - loss: 6114.6792 - val_loss: 7680.8550\n",
      "Epoch 36/100\n",
      "19/19 [==============================] - 0s 184us/step - loss: 5869.7783 - val_loss: 7518.3960\n",
      "Epoch 37/100\n",
      "19/19 [==============================] - 0s 163us/step - loss: 5938.2231 - val_loss: 7347.1274\n",
      "Epoch 38/100\n",
      "19/19 [==============================] - 0s 181us/step - loss: 5832.4590 - val_loss: 7169.4805\n",
      "Epoch 39/100\n",
      "19/19 [==============================] - 0s 169us/step - loss: 5674.8755 - val_loss: 6981.4780\n",
      "Epoch 40/100\n",
      "19/19 [==============================] - 0s 372us/step - loss: 5513.0415 - val_loss: 6788.6987\n",
      "Epoch 41/100\n",
      "19/19 [==============================] - 0s 290us/step - loss: 5371.8135 - val_loss: 6589.1748\n",
      "Epoch 42/100\n",
      "19/19 [==============================] - 0s 195us/step - loss: 5332.4097 - val_loss: 6386.0229\n",
      "Epoch 43/100\n",
      "19/19 [==============================] - 0s 186us/step - loss: 5058.2778 - val_loss: 6182.2388\n",
      "Epoch 44/100\n",
      "19/19 [==============================] - 0s 192us/step - loss: 5022.0688 - val_loss: 5978.4531\n",
      "Epoch 45/100\n",
      "19/19 [==============================] - 0s 318us/step - loss: 4954.7651 - val_loss: 5777.4707\n",
      "Epoch 46/100\n",
      "19/19 [==============================] - 0s 155us/step - loss: 4862.7588 - val_loss: 5580.8447\n",
      "Epoch 47/100\n",
      "19/19 [==============================] - 0s 168us/step - loss: 4924.0171 - val_loss: 5392.8042\n",
      "Epoch 48/100\n",
      "19/19 [==============================] - 0s 154us/step - loss: 4574.8071 - val_loss: 5212.3457\n",
      "Epoch 49/100\n",
      "19/19 [==============================] - 0s 206us/step - loss: 4628.6870 - val_loss: 5042.5049\n",
      "Epoch 50/100\n",
      "19/19 [==============================] - 0s 252us/step - loss: 4799.9160 - val_loss: 4887.3315\n",
      "Epoch 51/100\n",
      "19/19 [==============================] - 0s 139us/step - loss: 4630.6792 - val_loss: 4746.2285\n",
      "Epoch 52/100\n",
      "19/19 [==============================] - 0s 167us/step - loss: 4168.5483 - val_loss: 4616.3130\n",
      "Epoch 53/100\n",
      "19/19 [==============================] - 0s 196us/step - loss: 4493.9180 - val_loss: 4500.9355\n",
      "Epoch 54/100\n",
      "19/19 [==============================] - 0s 162us/step - loss: 4237.5918 - val_loss: 4396.9097\n",
      "Epoch 55/100\n",
      "19/19 [==============================] - 0s 175us/step - loss: 4041.7183 - val_loss: 4304.9224\n",
      "Epoch 56/100\n",
      "19/19 [==============================] - 0s 186us/step - loss: 4173.4072 - val_loss: 4223.8965\n",
      "Epoch 57/100\n",
      "19/19 [==============================] - 0s 211us/step - loss: 4032.8989 - val_loss: 4151.4463\n",
      "Epoch 58/100\n",
      "19/19 [==============================] - 0s 162us/step - loss: 3926.2610 - val_loss: 4086.0310\n",
      "Epoch 59/100\n",
      "19/19 [==============================] - 0s 320us/step - loss: 3758.3208 - val_loss: 4025.5835\n",
      "Epoch 60/100\n",
      "19/19 [==============================] - 0s 244us/step - loss: 3734.0461 - val_loss: 3969.9822\n",
      "Epoch 61/100\n",
      "19/19 [==============================] - 0s 280us/step - loss: 3743.8606 - val_loss: 3918.1313\n",
      "Epoch 62/100\n",
      "19/19 [==============================] - 0s 257us/step - loss: 3559.2505 - val_loss: 3868.6448\n",
      "Epoch 63/100\n",
      "19/19 [==============================] - 0s 299us/step - loss: 3713.8430 - val_loss: 3820.4775\n",
      "Epoch 64/100\n",
      "19/19 [==============================] - 0s 157us/step - loss: 3919.8005 - val_loss: 3773.4734\n",
      "Epoch 65/100\n",
      "19/19 [==============================] - 0s 147us/step - loss: 3947.1008 - val_loss: 3726.7493\n",
      "Epoch 66/100\n",
      "19/19 [==============================] - 0s 176us/step - loss: 3267.1467 - val_loss: 3678.9417\n",
      "Epoch 67/100\n",
      "19/19 [==============================] - 0s 180us/step - loss: 3558.0962 - val_loss: 3630.6543\n",
      "Epoch 68/100\n",
      "19/19 [==============================] - 0s 181us/step - loss: 3315.5107 - val_loss: 3582.0950\n",
      "Epoch 69/100\n",
      "19/19 [==============================] - 0s 209us/step - loss: 2949.2966 - val_loss: 3533.1484\n",
      "Epoch 70/100\n",
      "19/19 [==============================] - 0s 215us/step - loss: 3305.5662 - val_loss: 3484.9258\n",
      "Epoch 71/100\n",
      "19/19 [==============================] - 0s 182us/step - loss: 3286.3438 - val_loss: 3435.3855\n",
      "Epoch 72/100\n",
      "19/19 [==============================] - 0s 166us/step - loss: 3099.7302 - val_loss: 3386.3337\n",
      "Epoch 73/100\n",
      "19/19 [==============================] - 0s 154us/step - loss: 2997.1794 - val_loss: 3338.4780\n",
      "Epoch 74/100\n",
      "19/19 [==============================] - 0s 165us/step - loss: 2723.5754 - val_loss: 3291.2031\n",
      "Epoch 75/100\n",
      "19/19 [==============================] - 0s 220us/step - loss: 3137.9775 - val_loss: 3243.4209\n",
      "Epoch 76/100\n",
      "19/19 [==============================] - 0s 158us/step - loss: 2568.5156 - val_loss: 3198.5183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "19/19 [==============================] - 0s 202us/step - loss: 2971.1602 - val_loss: 3157.0210\n",
      "Epoch 78/100\n",
      "19/19 [==============================] - 0s 182us/step - loss: 2861.9768 - val_loss: 3117.9336\n",
      "Epoch 79/100\n",
      "19/19 [==============================] - 0s 158us/step - loss: 2486.0098 - val_loss: 3077.3369\n",
      "Epoch 80/100\n",
      "19/19 [==============================] - 0s 158us/step - loss: 2375.9895 - val_loss: 3036.9915\n",
      "Epoch 81/100\n",
      "19/19 [==============================] - 0s 167us/step - loss: 2703.7979 - val_loss: 2999.3020\n",
      "Epoch 82/100\n",
      "19/19 [==============================] - 0s 172us/step - loss: 2628.5889 - val_loss: 2968.1982\n",
      "Epoch 83/100\n",
      "19/19 [==============================] - 0s 143us/step - loss: 2407.0864 - val_loss: 2941.9795\n",
      "Epoch 84/100\n",
      "19/19 [==============================] - 0s 201us/step - loss: 2277.7139 - val_loss: 2919.7126\n",
      "Epoch 85/100\n",
      "19/19 [==============================] - 0s 158us/step - loss: 2074.0771 - val_loss: 2902.2729\n",
      "Epoch 86/100\n",
      "19/19 [==============================] - 0s 148us/step - loss: 2705.6084 - val_loss: 2888.9978\n",
      "Epoch 87/100\n",
      "19/19 [==============================] - 0s 144us/step - loss: 1920.0679 - val_loss: 2881.7664\n",
      "Epoch 88/100\n",
      "19/19 [==============================] - 0s 184us/step - loss: 2250.8132 - val_loss: 2873.4702\n",
      "Epoch 89/100\n",
      "19/19 [==============================] - 0s 126us/step - loss: 1972.1891 - val_loss: 2870.5591\n",
      "Epoch 90/100\n",
      "19/19 [==============================] - 0s 155us/step - loss: 1663.6621 - val_loss: 2872.9463\n",
      "Epoch 91/100\n",
      "19/19 [==============================] - 0s 135us/step - loss: 1849.7679 - val_loss: 2870.8860\n",
      "Epoch 92/100\n",
      "19/19 [==============================] - 0s 161us/step - loss: 1655.2601 - val_loss: 2868.4663\n",
      "Epoch 93/100\n",
      "19/19 [==============================] - 0s 149us/step - loss: 1866.7948 - val_loss: 2861.9758\n",
      "Epoch 94/100\n",
      "19/19 [==============================] - 0s 155us/step - loss: 1932.0205 - val_loss: 2853.5535\n",
      "Epoch 95/100\n",
      "19/19 [==============================] - 0s 226us/step - loss: 1618.5970 - val_loss: 2841.6990\n",
      "Epoch 96/100\n",
      "19/19 [==============================] - 0s 138us/step - loss: 1898.1163 - val_loss: 2827.6726\n",
      "Epoch 97/100\n",
      "19/19 [==============================] - 0s 205us/step - loss: 1856.8029 - val_loss: 2807.0759\n",
      "Epoch 98/100\n",
      "19/19 [==============================] - 0s 203us/step - loss: 1828.0745 - val_loss: 2788.4028\n",
      "Epoch 99/100\n",
      "19/19 [==============================] - 0s 693us/step - loss: 1811.4712 - val_loss: 2778.5376\n",
      "Epoch 100/100\n",
      "19/19 [==============================] - 0s 265us/step - loss: 1334.0288 - val_loss: 2772.0066\n",
      "2/2 [==============================] - 0s 587us/step\n",
      "29.805227100292324\n",
      "Saved deep learning model as 'dl_distance_high_model.json'\n"
     ]
    }
   ],
   "source": [
    "dl_model_distance = deep_learning(X_distance_high, y_distance_high, keras_distance_high=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 130541 samples, validate on 6871 samples\n",
      "Epoch 1/100\n",
      "130541/130541 [==============================] - 8s 64us/step - loss: 22.2949 - val_loss: 16.2265\n",
      "Epoch 2/100\n",
      "130541/130541 [==============================] - 7s 55us/step - loss: 18.1674 - val_loss: 15.8544\n",
      "Epoch 3/100\n",
      "130541/130541 [==============================] - 8s 59us/step - loss: 17.7983 - val_loss: 15.6216\n",
      "Epoch 4/100\n",
      "130541/130541 [==============================] - 7s 57us/step - loss: 17.3902 - val_loss: 15.5995\n",
      "Epoch 5/100\n",
      "130541/130541 [==============================] - 7s 56us/step - loss: 17.0627 - val_loss: 15.6161\n",
      "Epoch 6/100\n",
      "130541/130541 [==============================] - 7s 57us/step - loss: 16.9101 - val_loss: 15.1648\n",
      "Epoch 7/100\n",
      "130541/130541 [==============================] - 9s 66us/step - loss: 16.6655 - val_loss: 15.0737\n",
      "Epoch 8/100\n",
      "130541/130541 [==============================] - 8s 63us/step - loss: 16.4252 - val_loss: 14.8600\n",
      "Epoch 9/100\n",
      "130541/130541 [==============================] - 7s 55us/step - loss: 16.2186 - val_loss: 15.0469\n",
      "Epoch 10/100\n",
      "130541/130541 [==============================] - 7s 56us/step - loss: 16.0593 - val_loss: 15.0859\n",
      "Epoch 11/100\n",
      "130541/130541 [==============================] - 7s 55us/step - loss: 15.8336 - val_loss: 15.0798\n",
      "7233/7233 [==============================] - 0s 27us/step\n",
      "3.810530707506849\n",
      "Saved deep learning model as 'dl_model.json'\n"
     ]
    }
   ],
   "source": [
    "dl_model = deep_learning(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Loaded model from disk\n",
      "Loaded model from disk\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-27 13:08:24.0000002</td>\n",
       "      <td>14.872206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-27 13:08:24.0000003</td>\n",
       "      <td>15.555851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-08 11:53:44.0000002</td>\n",
       "      <td>7.803155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-01 21:12:12.0000002</td>\n",
       "      <td>11.386860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-01 21:12:12.0000003</td>\n",
       "      <td>18.263371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012-12-01 21:12:12.0000005</td>\n",
       "      <td>14.453194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2011-10-06 12:10:20.0000001</td>\n",
       "      <td>9.671371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2011-10-06 12:10:20.0000003</td>\n",
       "      <td>58.028358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2011-10-06 12:10:20.0000002</td>\n",
       "      <td>16.559309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2014-02-18 15:22:20.0000002</td>\n",
       "      <td>10.938480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2014-02-18 15:22:20.0000003</td>\n",
       "      <td>14.670376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2014-02-18 15:22:20.0000001</td>\n",
       "      <td>21.036930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2010-03-29 20:20:32.0000002</td>\n",
       "      <td>6.779317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2010-03-29 20:20:32.0000001</td>\n",
       "      <td>9.408459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2011-10-06 03:59:12.0000002</td>\n",
       "      <td>10.780025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2011-10-06 03:59:12.0000001</td>\n",
       "      <td>15.747732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2012-07-15 16:45:04.0000006</td>\n",
       "      <td>7.502507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012-07-15 16:45:04.0000002</td>\n",
       "      <td>12.387024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2012-07-15 16:45:04.0000003</td>\n",
       "      <td>7.984553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2012-07-15 16:45:04.0000004</td>\n",
       "      <td>7.483736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2014-10-29 02:09:56.0000001</td>\n",
       "      <td>12.424219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2014-06-14 13:39:00.00000010</td>\n",
       "      <td>13.506950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2014-06-14 13:39:00.00000060</td>\n",
       "      <td>10.502642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2014-06-14 13:39:00.00000087</td>\n",
       "      <td>12.914953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2014-06-14 13:39:00.00000050</td>\n",
       "      <td>20.591101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2014-06-14 13:39:00.0000003</td>\n",
       "      <td>10.663095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2014-06-14 13:39:00.000000158</td>\n",
       "      <td>44.662579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2014-06-14 13:39:00.00000015</td>\n",
       "      <td>26.896694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2014-06-14 13:39:00.00000073</td>\n",
       "      <td>9.603252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2014-06-14 13:39:00.00000077</td>\n",
       "      <td>19.367861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>2010-08-14 02:13:00.0000003</td>\n",
       "      <td>17.595676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7198</th>\n",
       "      <td>2010-09-29 16:43:00.000000172</td>\n",
       "      <td>11.630340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7322</th>\n",
       "      <td>2011-10-04 09:37:00.00000077</td>\n",
       "      <td>6.119107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7526</th>\n",
       "      <td>2011-03-06 21:01:00.00000081</td>\n",
       "      <td>20.503082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>2012-01-26 07:33:00.000000136</td>\n",
       "      <td>10.689713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>2012-01-26 07:33:00.00000023</td>\n",
       "      <td>10.751810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7637</th>\n",
       "      <td>2010-12-09 07:29:00.00000083</td>\n",
       "      <td>7.727076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7839</th>\n",
       "      <td>2012-12-15 06:35:45.0000001</td>\n",
       "      <td>11.115673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7874</th>\n",
       "      <td>2009-06-02 08:34:33.0000001</td>\n",
       "      <td>7.905365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>2013-01-19 20:19:45.0000002</td>\n",
       "      <td>7.376270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8159</th>\n",
       "      <td>2011-07-20 08:05:02.0000001</td>\n",
       "      <td>10.330705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381</th>\n",
       "      <td>2009-07-30 15:49:15.0000002</td>\n",
       "      <td>12.114047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8424</th>\n",
       "      <td>2009-06-10 16:55:00.000000155</td>\n",
       "      <td>6.375244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8426</th>\n",
       "      <td>2011-06-24 12:03:00.000000131</td>\n",
       "      <td>8.689192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8517</th>\n",
       "      <td>2011-03-11 01:44:00.00000067</td>\n",
       "      <td>9.266556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596</th>\n",
       "      <td>2010-09-20 16:48:00.00000021</td>\n",
       "      <td>10.863045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8632</th>\n",
       "      <td>2009-10-29 10:48:40.0000003</td>\n",
       "      <td>5.124941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8835</th>\n",
       "      <td>2012-12-01 21:12:12.0000001</td>\n",
       "      <td>8.212973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9006</th>\n",
       "      <td>2013-07-02 22:27:14.0000001</td>\n",
       "      <td>7.559417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221</th>\n",
       "      <td>2010-08-27 18:45:00.000000215</td>\n",
       "      <td>7.916724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9383</th>\n",
       "      <td>2011-03-11 01:44:00.00000064</td>\n",
       "      <td>8.918550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9421</th>\n",
       "      <td>2011-10-04 09:37:00.00000088</td>\n",
       "      <td>5.426321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9478</th>\n",
       "      <td>2011-12-13 22:00:00.00000044</td>\n",
       "      <td>7.758844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>2014-07-21 18:19:00.00000025</td>\n",
       "      <td>5.738677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9563</th>\n",
       "      <td>2011-03-06 21:01:00.00000018</td>\n",
       "      <td>8.105506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9830</th>\n",
       "      <td>2014-07-21 18:19:00.00000065</td>\n",
       "      <td>5.478869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9888</th>\n",
       "      <td>2013-09-25 22:00:00.000000146</td>\n",
       "      <td>8.719370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>2010-06-11 13:37:21.0000004</td>\n",
       "      <td>48.410961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5887</th>\n",
       "      <td>2010-07-04 16:44:11.0000002</td>\n",
       "      <td>77.291222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8529</th>\n",
       "      <td>2009-11-25 19:32:52.0000001</td>\n",
       "      <td>43.079468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9914 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                key  fare_amount\n",
       "0       2015-01-27 13:08:24.0000002    14.872206\n",
       "1       2015-01-27 13:08:24.0000003    15.555851\n",
       "2       2011-10-08 11:53:44.0000002     7.803155\n",
       "3       2012-12-01 21:12:12.0000002    11.386860\n",
       "4       2012-12-01 21:12:12.0000003    18.263371\n",
       "5       2012-12-01 21:12:12.0000005    14.453194\n",
       "6       2011-10-06 12:10:20.0000001     9.671371\n",
       "7       2011-10-06 12:10:20.0000003    58.028358\n",
       "8       2011-10-06 12:10:20.0000002    16.559309\n",
       "9       2014-02-18 15:22:20.0000002    10.938480\n",
       "10      2014-02-18 15:22:20.0000003    14.670376\n",
       "11      2014-02-18 15:22:20.0000001    21.036930\n",
       "12      2010-03-29 20:20:32.0000002     6.779317\n",
       "13      2010-03-29 20:20:32.0000001     9.408459\n",
       "14      2011-10-06 03:59:12.0000002    10.780025\n",
       "15      2011-10-06 03:59:12.0000001    15.747732\n",
       "16      2012-07-15 16:45:04.0000006     7.502507\n",
       "17      2012-07-15 16:45:04.0000002    12.387024\n",
       "18      2012-07-15 16:45:04.0000003     7.984553\n",
       "19      2012-07-15 16:45:04.0000004     7.483736\n",
       "20      2014-10-29 02:09:56.0000001    12.424219\n",
       "21     2014-06-14 13:39:00.00000010    13.506950\n",
       "22     2014-06-14 13:39:00.00000060    10.502642\n",
       "23     2014-06-14 13:39:00.00000087    12.914953\n",
       "24     2014-06-14 13:39:00.00000050    20.591101\n",
       "25      2014-06-14 13:39:00.0000003    10.663095\n",
       "26    2014-06-14 13:39:00.000000158    44.662579\n",
       "27     2014-06-14 13:39:00.00000015    26.896694\n",
       "28     2014-06-14 13:39:00.00000073     9.603252\n",
       "29     2014-06-14 13:39:00.00000077    19.367861\n",
       "...                             ...          ...\n",
       "7163    2010-08-14 02:13:00.0000003    17.595676\n",
       "7198  2010-09-29 16:43:00.000000172    11.630340\n",
       "7322   2011-10-04 09:37:00.00000077     6.119107\n",
       "7526   2011-03-06 21:01:00.00000081    20.503082\n",
       "7607  2012-01-26 07:33:00.000000136    10.689713\n",
       "7610   2012-01-26 07:33:00.00000023    10.751810\n",
       "7637   2010-12-09 07:29:00.00000083     7.727076\n",
       "7839    2012-12-15 06:35:45.0000001    11.115673\n",
       "7874    2009-06-02 08:34:33.0000001     7.905365\n",
       "7994    2013-01-19 20:19:45.0000002     7.376270\n",
       "8159    2011-07-20 08:05:02.0000001    10.330705\n",
       "8381    2009-07-30 15:49:15.0000002    12.114047\n",
       "8424  2009-06-10 16:55:00.000000155     6.375244\n",
       "8426  2011-06-24 12:03:00.000000131     8.689192\n",
       "8517   2011-03-11 01:44:00.00000067     9.266556\n",
       "8596   2010-09-20 16:48:00.00000021    10.863045\n",
       "8632    2009-10-29 10:48:40.0000003     5.124941\n",
       "8835    2012-12-01 21:12:12.0000001     8.212973\n",
       "9006    2013-07-02 22:27:14.0000001     7.559417\n",
       "9221  2010-08-27 18:45:00.000000215     7.916724\n",
       "9383   2011-03-11 01:44:00.00000064     8.918550\n",
       "9421   2011-10-04 09:37:00.00000088     5.426321\n",
       "9478   2011-12-13 22:00:00.00000044     7.758844\n",
       "9500   2014-07-21 18:19:00.00000025     5.738677\n",
       "9563   2011-03-06 21:01:00.00000018     8.105506\n",
       "9830   2014-07-21 18:19:00.00000065     5.478869\n",
       "9888  2013-09-25 22:00:00.000000146     8.719370\n",
       "4080    2010-06-11 13:37:21.0000004    48.410961\n",
       "5887    2010-07-04 16:44:11.0000002    77.291222\n",
       "8529    2009-11-25 19:32:52.0000001    43.079468\n",
       "\n",
       "[9914 rows x 2 columns]"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_submit(y_test, 'dl_model.json', 'dl_distance_none_model.json', 'dl_distance_high_model.json', keras=True, keras_distance_none=True, keras_distance_high=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  rmse scores: [13.95366771 10.9274019  12.08455299 11.87294842 10.76575251]\n",
      "Random Forest  mean score: 11.920864707032875\n",
      "Random Forest  std: 1.1386509922108163\n",
      "Linear Regression model saved as \"rf_distance_none_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=10, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest(X_distance_none, y_distance_none, distance_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  rmse scores: [ 52.23445375  50.11460938 103.85570503  37.84936334  39.21845223]\n",
      "Random Forest  mean score: 56.6545167464469\n",
      "Random Forest  std: 24.281308288210067\n",
      "Linear Regression model saved as \"rf_distance_high_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=10, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest(X_distance_high, y_distance_high, distance_high=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  rmse scores: [3.56023513 3.51308453 3.96715795 3.8646105  3.61284635]\n",
      "Random Forest  mean score: 3.7035868917605996\n",
      "Random Forest  std: 0.1791496768716599\n",
      "Linear Regression model saved as \"rf_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=10, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.34330165502726584, 'euclidean_distance'),\n",
       " (0.23897415208929706, 'taxicab_distance'),\n",
       " (0.08441907303610358, 'manhattan'),\n",
       " (0.06531669279188136, 'jfk'),\n",
       " (0.05976043110506831, 'pickup_longitude'),\n",
       " (0.055038741636194814, 'dropoff_longitude'),\n",
       " (0.03242513721193783, 'pickup_latitude'),\n",
       " (0.03219713258788817, 'dropoff_latitude'),\n",
       " (0.02303074019291758, 'year'),\n",
       " (0.016137066545245386, 'total_seconds'),\n",
       " (0.008105294794532845, 'newark'),\n",
       " (0.004474721982032309, 'manhattan_one_way'),\n",
       " (0.003316634099154293, 'night_charge'),\n",
       " (0.0032091701562175657, 'passenger_count'),\n",
       " (0.0021644301276920706, 'Sat'),\n",
       " (0.0019202045722021012, 'Oct'),\n",
       " (0.0018882711467735345, 'Sep'),\n",
       " (0.0018503660865519366, 'Fri'),\n",
       " (0.0018303735799151438, 'Wed'),\n",
       " (0.0018211300914307976, 'Thu'),\n",
       " (0.0015782242219240872, 'Tue'),\n",
       " (0.0015314676243699836, 'Jul'),\n",
       " (0.0015131065067303037, 'Mar'),\n",
       " (0.0015025134376371431, 'weekday_surcharge'),\n",
       " (0.0013360816663071375, 'Sun'),\n",
       " (0.0013082388107069373, 'Dec'),\n",
       " (0.0012987176545774717, 'Mon'),\n",
       " (0.0012104366050683833, 'morning_rush'),\n",
       " (0.0011677542889346648, 'Apr'),\n",
       " (0.001150213200384346, 'Nov'),\n",
       " (0.0011387283972948668, 'Jan'),\n",
       " (0.0010842884440028155, 'Jun'),\n",
       " (0.0010233365997063145, 'May'),\n",
       " (0.000993801995537515, 'Aug'),\n",
       " (0.0009816716865155152, 'Feb')]"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = open_model('rf_model.pkl')\n",
    "feature_importances = rf.feature_importances_\n",
    "sorted(zip(feature_importances, list(X)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda\n",
    "# p42xlarge\n",
    "# ec2 instance pricing\n",
    "# make sure you have gpu and optimization\n",
    "# save as pickle file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
