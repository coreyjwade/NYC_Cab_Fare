{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_dataFrame(file_name, subset=True, nrows=1000000):\n",
    "    if subset:\n",
    "        df = pd.read_csv(file_name, nrows=nrows, parse_dates=['pickup_datetime'])\n",
    "    else:\n",
    "        df = pd.read_csv(file_name, parse_dates=['pickup_datetime'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xtest_ytest(df):\n",
    "    y_test = df['key']\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    X_test = df.drop('key', axis=1)\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    print('dropping nan:', len(df))\n",
    "    df = df.dropna(axis=0, subset=['dropoff_latitude'])\n",
    "    df = df.drop('key', axis=1)\n",
    "    print('nan dropped:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_US(df):\n",
    "    # Choose cab rides whose pickup and dropoff are the US Mainland\n",
    "    # Declare constants\n",
    "    latmin = 5.496100\n",
    "    latmax = 71.538800\n",
    "    longmin = -124.482003\n",
    "    longmax = -66.885417\n",
    "\n",
    "    # Create dataframe with correct coordinates\n",
    "    df = df[((((df['pickup_longitude']<=longmax) & (df['pickup_longitude']>=longmin)) & ((df['pickup_latitude']<=latmax) & (df['pickup_latitude']>=latmin)))) & ((((df['dropoff_longitude']<=longmax) & (df['dropoff_longitude']>=longmin)) & ((df['dropoff_latitude']<=latmax) & (df['dropoff_latitude']>=latmin))))]\n",
    "    \n",
    "    print('US Mainland Only dropped:', len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_NYC(df):\n",
    "    # Find cab rides whose pickup or dropoff are within NYC boundaries\n",
    "    # Declare constants\n",
    "    latmin = 40.477399\n",
    "    latmax = 40.917577\n",
    "    longmin = -74.259090\n",
    "    longmax = -73.700272\n",
    "\n",
    "    # Create dataframe with correct coordinates\n",
    "    df = df[((((df['pickup_longitude']<=longmax) & (df['pickup_longitude']>=longmin)) & ((df['pickup_latitude']<=latmax) & (df['pickup_latitude']>=latmin)))) | ((((df['dropoff_longitude']<=longmax) % (df['dropoff_longitude']>=longmin)) & ((df['dropoff_latitude']<=latmax) & (df['dropoff_latitude']>=latmin))))]\n",
    "    \n",
    "    print('NYC Taxis Only:', len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_Riders(df, num=7):\n",
    "    # Only choose cabs between 1 and num riders\n",
    "    df = df[(df['passenger_count'] <= num) & (df['passenger_count'] > 0)]\n",
    "    print('Max Passengers 7:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance(df):\n",
    "\n",
    "    # Define coordinates (x,y)\n",
    "    x1 = df['pickup_latitude']\n",
    "    y1 = df['pickup_longitude']\n",
    "    x2 = df['dropoff_latitude']\n",
    "    y2 = df['dropoff_longitude']\n",
    "\n",
    "    # Create Euclidean Distrance column\n",
    "    df['euclidean_distance'] = np.sqrt((y2-y1)**2 + (x2-x1)**2)\n",
    "\n",
    "    # Create Taxicab Distance column\n",
    "    df['taxicab_distance'] = np.abs(y2-y1) + np.abs(x2-x1)\n",
    "\n",
    "    # Convert to miles\n",
    "    df['euclidean_distance'] = df['euclidean_distance'] * 69\n",
    "    df['taxicab_distance'] = df['taxicab_distance'] * 69\n",
    "    \n",
    "    print('Distance Columns added...')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_Fare(df):\n",
    "    # Eliminate unrealistic plots\n",
    "    df = df[df['fare_amount'] >= (df['euclidean_distance'] * 2 + 2.5)]\n",
    "    print('Min fares dropped:', len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_Fare(df):\n",
    "    df = df[(df['fare_amount'] <= (df['taxicab_distance'] * 48 + 16)) | (df['fare_amount'] <= 56)]\n",
    "    print('Max fares dropped:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_distance(df):\n",
    "    # Elminate fares that traveled no distance\n",
    "    df = df[df['euclidean_distance']>0]\n",
    "    print('No distance dropped:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_elimination(df):\n",
    "    df = clean_data(df)\n",
    "    df = lat_lon_US(df)\n",
    "    df = lat_lon_NYC(df)\n",
    "    df = max_Riders(df)\n",
    "    df = add_distance(df)\n",
    "    df = min_Fare(df)\n",
    "    df = max_Fare(df)\n",
    "    df = no_distance(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_train, y_train Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_y(df):\n",
    "    X = df.drop('fare_amount', axis=1)\n",
    "    y = df['fare_amount'].copy()\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garbage Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of accumulated garbage\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Time_units(df):\n",
    "    \n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['minute'] = df['pickup_datetime'].dt.minute\n",
    "    df['second'] = df['pickup_datetime'].dt.second\n",
    "    df['dayofweek'] = df['pickup_datetime'].dt.dayofweek\n",
    "    \n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "    dr = pd.date_range(start='2009-01-01', end='2015-12-31')\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "    df['holiday'] = df['pickup_datetime'].dt.date.astype('datetime64').isin(holidays)\n",
    "    \n",
    "    df = df.drop('pickup_datetime', axis=1)\n",
    "\n",
    "    df['15_min_intervals'] = 4 * df['hour'] + (df['minute']/15).astype(int)\n",
    "    df['total_seconds'] = 3600 * df['hour'] + 60 * df['minute'] + df['second']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Time_columns(df):\n",
    "\n",
    "    def summer_month(row):\n",
    "        if row['month'] in [6,7,8]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    df['summer_month'] = df.apply(summer_month, axis=1)\n",
    "    \n",
    "    \n",
    "    def cold_month(row):\n",
    "        if row['month'] in [1,2,3,11,12]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    df['cold_month'] = df.apply(cold_month, axis=1)\n",
    "    \n",
    "    def weekend(row):\n",
    "        if (row['dayofweek'] in [5,6]) | (row['holiday']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['weekend'] = df.apply(weekend, axis=1)\n",
    "\n",
    "    def rush_hour(row):\n",
    "        if ((row['hour'] in [7,8,9,15,16,17,18,19]) & (row['weekend'] == 0)) & (not row['holiday']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['rush_hour'] = df.apply(rush_hour, axis=1)\n",
    "    \n",
    "    def morning_rush(row):\n",
    "        if ((row['hour'] in [6,7,8,9]) & (row['weekend'] == 0)) & (not row['holiday']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['morning_rush'] = df.apply(morning_rush, axis=1)\n",
    "\n",
    "    def night_rush(row):\n",
    "        if (row['hour'] in [19,20,21,22,23,24,1]) & (row['dayofweek'] in [3,4,5]):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    df['night_rush'] = df.apply(night_rush, axis=1)\n",
    "\n",
    "    def night_charge(row):\n",
    "        if row['hour'] in [20,21,22,23,24,1,2,3,4,5,6]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['night_charge'] = df.apply(night_charge, axis=1)\n",
    "\n",
    "    def weekday_surcharge(row):\n",
    "        if ((row['hour'] in [16,17,18,19,20]) & (row['dayofweek'] in [0,1,2,3,4])) & (not row['holiday']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['weekday_surcharge'] = df.apply(weekday_surcharge, axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Time(df):\n",
    "    df = add_Time_units(df)\n",
    "    df = add_Time_columns(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define line from two points and a provided column\n",
    "def two_points_line(a, b, column):\n",
    "        \n",
    "    # Case when y-values are the same\n",
    "    if b[1]==a[1]:\n",
    "        \n",
    "        # Slope defaults to 0\n",
    "        slope = 0\n",
    "        \n",
    "    # Case when x-values are the same\n",
    "    elif b[0]==a[0]:\n",
    "        \n",
    "        # Case when max value is less than 999999999\n",
    "        if column.max() < 999999999:\n",
    "            \n",
    "            # Add 999999999 to max value\n",
    "            slope = column.max() + 999999999\n",
    "        \n",
    "        # All other cases\n",
    "        else:\n",
    "            \n",
    "            # Multiply max value by itself (greater than 999999999)\n",
    "            slope = column.max() * column.max()\n",
    "    \n",
    "    # When x-values and y-values are not 0\n",
    "    else:\n",
    "        \n",
    "        # Use standard slope formula\n",
    "        slope = (b[1] - a[1])/(b[0]-a[0])\n",
    "    \n",
    "    \n",
    "    # Equation for y-intercept (solving y=mx+b for b)\n",
    "    y_int = a[1] - slope * a[0]\n",
    "    \n",
    "    # Return slope and y-intercept\n",
    "    return slope, y_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_cols(df):\n",
    "    \n",
    "    upper_right = (-73.929224, 40.804328)\n",
    "    bottom_right = (-73.980036, 40.710706)\n",
    "    bottom_left = (-74.054880, 40.681292)\n",
    "    upper_left = (-73.966303, 40.830050)\n",
    "\n",
    "    m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "    m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "    m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "    m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "    def manhattan(row):\n",
    "        if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "        (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['pickup_latitude'] >= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "        (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))) & (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "        (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "        (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    df['manhattan'] = df.apply(manhattan, axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newark_cols(df):\n",
    "    \n",
    "    upper_right = (-74.107867, 40.718282)\n",
    "    bottom_right = (-74.143665, 40.654673)\n",
    "    bottom_left = (-74.250524, 40.698436)\n",
    "    upper_left = (-74.171983, 40.792347)\n",
    "\n",
    "    m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "    m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "    m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "    m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "    def newark(row):\n",
    "        if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "        (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['pickup_latitude'] >= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "        (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))) | (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "        (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "        (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    df['newark'] = df.apply(newark, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jkf_cols(df):\n",
    "    \n",
    "    upper_right = (-73.789700, 40.663781)\n",
    "    bottom_right = (-73.762112, 40.633567)\n",
    "    bottom_left = (-73.818920, 40.642250)\n",
    "    upper_left = (-73.804656, 40.664858)\n",
    "\n",
    "    m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "    m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "    m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "    m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "    def jfk(row):\n",
    "        if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "        (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['pickup_latitude'] <= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "        (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))) | (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "        (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "        (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    df['jfk'] = df.apply(jfk, axis=1)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_locations(df):\n",
    "    df = manhattan_cols(df)\n",
    "    df = jkf_cols(df)\n",
    "    df = newark_cols(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cols(df):\n",
    "    df = add_Time(df)\n",
    "    df = add_locations(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_predictor_cols(df, cols=['month', 'year', 'dayofweek', 'total_seconds', '15_min_intervals', 'weekend', 'morning_rush', 'night_charge', 'weekday_surcharge', 'manhattan', 'jfk', 'newark', 'passenger_count','euclidean_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']):\n",
    "    X = df[cols]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_cols(X):\n",
    "    X = one_Hot_Encoder(X, X['month'])\n",
    "    X = one_Hot_Encoder(X, X['dayofweek'], month=False)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_Hot_Encoder(X, col, month=True): \n",
    "    encoder = OneHotEncoder()\n",
    "    col = encoder.fit_transform(np.array(col).reshape(-1,1))\n",
    "    col = col.toarray()\n",
    "    hot_df = pd.DataFrame(col)\n",
    "    if month:\n",
    "        hot_df.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    else:\n",
    "        hot_df.columns = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "    new_df = X.join(hot_df)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pipeline(test_set=False):\n",
    "    \n",
    "    if test_set:\n",
    "        df = file_to_dataFrame('test.csv')\n",
    "        X, y = make_Xtest_ytest(df)\n",
    "        X = add_distance(X)\n",
    "    \n",
    "    else:\n",
    "        df = file_to_dataFrame('train.csv')\n",
    "        df = row_elimination(df)\n",
    "        X, y = make_X_y(df)\n",
    "    \n",
    "    X = add_cols(X)\n",
    "    X = choose_predictor_cols(X)\n",
    "    X = min_max_scaler(X)\n",
    "    X = one_Hot_Encoder(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping nan: 1000000\n",
      "nan dropped: 999990\n",
      "US Mainland Only dropped: 979520\n",
      "NYC Taxis Only: 978430\n",
      "Max Passengers 7: 974949\n",
      "Distance Columns added...\n",
      "Min fares dropped: 946884\n",
      "Max fares dropped: 946364\n",
      "No distance dropped: 936346\n",
      "Distance Columns added...\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = my_pipeline()\n",
    "X_test, y_test = my_pipeline(test_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y(model, X_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_to_kaggle(y_pred):    \n",
    "    y_test['fare_amount'] = y_pred\n",
    "    y_test.to_csv('my_submission.csv', index=False)\n",
    "    print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_test(X_train, y_train, X_test):\n",
    "        \n",
    "    print('Length of X:', len(X_train))\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse = np.sqrt(-scores)\n",
    "    print('Lin reg train rmse:', rmse)\n",
    "    print('Lin reg train mean:', rmse.mean())\n",
    "    print('Lin reg train std:', rmse.std())\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    submit_to_kaggle(y_pred)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 140311\n",
      "Lin reg train rmse: [3.53290023 3.72230434 3.73733944 3.54865963 3.61891226]\n",
      "Lin reg train mean: 3.6320231786015547\n",
      "Lin reg train std: 0.08507519076721042\n",
      "                                key  fare_amount\n",
      "0       2015-01-27 13:08:24.0000002    10.544257\n",
      "1       2015-01-27 13:08:24.0000003    10.795924\n",
      "2       2011-10-08 11:53:44.0000002     6.068465\n",
      "3       2012-12-01 21:12:12.0000002     8.237116\n",
      "4       2012-12-01 21:12:12.0000003    13.988656\n",
      "5       2012-12-01 21:12:12.0000005    10.424865\n",
      "6       2011-10-06 12:10:20.0000001     7.426294\n",
      "7       2011-10-06 12:10:20.0000003    48.532370\n",
      "8       2011-10-06 12:10:20.0000002    12.020170\n",
      "9       2014-02-18 15:22:20.0000002     8.352579\n",
      "10      2014-02-18 15:22:20.0000003    10.321574\n",
      "11      2014-02-18 15:22:20.0000001    15.121286\n",
      "12      2010-03-29 20:20:32.0000002     4.802007\n",
      "13      2010-03-29 20:20:32.0000001     7.335894\n",
      "14      2011-10-06 03:59:12.0000002     8.186905\n",
      "15      2011-10-06 03:59:12.0000001    12.379089\n",
      "16      2012-07-15 16:45:04.0000006     6.107976\n",
      "17      2012-07-15 16:45:04.0000002     9.848033\n",
      "18      2012-07-15 16:45:04.0000003     6.516026\n",
      "19      2012-07-15 16:45:04.0000004     6.201212\n",
      "20      2014-10-29 02:09:56.0000001    10.272394\n",
      "21     2014-06-14 13:39:00.00000010     9.824187\n",
      "22     2014-06-14 13:39:00.00000060     8.225650\n",
      "23     2014-06-14 13:39:00.00000087     9.156164\n",
      "24     2014-06-14 13:39:00.00000050    15.958274\n",
      "25      2014-06-14 13:39:00.0000003     8.242176\n",
      "26    2014-06-14 13:39:00.000000158    33.350482\n",
      "27     2014-06-14 13:39:00.00000015    21.249547\n",
      "28     2014-06-14 13:39:00.00000073     7.687949\n",
      "29     2014-06-14 13:39:00.00000077    14.549476\n",
      "...                             ...          ...\n",
      "9884   2013-09-25 22:00:00.00000060    30.875520\n",
      "9885  2013-09-25 22:00:00.000000213    14.725295\n",
      "9886  2013-09-25 22:00:00.000000150    19.114587\n",
      "9887   2013-09-25 22:00:00.00000010     9.524756\n",
      "9888  2013-09-25 22:00:00.000000146     5.652366\n",
      "9889   2013-09-25 22:00:00.00000041    11.630173\n",
      "9890  2013-09-25 22:00:00.000000109    10.946841\n",
      "9891  2013-09-25 22:00:00.000000210    16.001663\n",
      "9892  2013-09-25 22:00:00.000000151    10.484273\n",
      "9893  2013-09-25 22:00:00.000000190    15.110544\n",
      "9894  2013-09-25 22:00:00.000000153    10.934729\n",
      "9895  2013-09-25 22:00:00.000000241    22.627857\n",
      "9896  2013-09-25 22:00:00.000000127    10.907736\n",
      "9897    2015-02-20 11:08:29.0000001    15.462464\n",
      "9898    2015-01-12 15:36:37.0000002     7.648291\n",
      "9899    2015-06-07 00:38:14.0000002    19.124549\n",
      "9900    2015-04-12 21:56:22.0000005     9.075172\n",
      "9901    2015-04-10 11:56:54.0000004     9.690813\n",
      "9902    2015-06-25 01:01:46.0000002    14.847610\n",
      "9903    2015-05-29 10:02:42.0000001    11.274703\n",
      "9904    2015-06-30 20:03:50.0000002    43.315842\n",
      "9905    2015-02-27 19:36:02.0000006    24.076006\n",
      "9906    2015-06-15 01:00:06.0000002     6.343138\n",
      "9907    2015-02-03 09:00:58.0000001    31.762402\n",
      "9908    2015-05-19 13:58:11.0000001     9.924938\n",
      "9909    2015-05-10 12:37:51.0000002    10.247202\n",
      "9910    2015-01-12 17:05:51.0000001    12.167508\n",
      "9911    2015-04-19 20:44:15.0000001    45.377555\n",
      "9912    2015-01-31 01:05:19.0000005    18.625350\n",
      "9913    2015-01-18 14:06:23.0000006     9.038184\n",
      "\n",
      "[9914 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "lin_reg_model = lin_reg_test(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_regression_test requires \"from sklearn.model_selection import train_test_split\"\n",
    "def keras_regression_test(X_train, y_train, X_test, numbers=[128,64], batch_size=32, activation='relu', optimizer='adam', loss='mean_squared_error'):\n",
    "        \n",
    "    #X_train, X_check, y_train, y_check = train_test_split(X, y)\n",
    "    \n",
    "    # Save the number of columns in predictors: n_cols\n",
    "    n_cols = X_train.shape[1]\n",
    "\n",
    "    # Set up the model: model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add the first layer\n",
    "    model.add(Dense(numbers[0], activation=activation, input_shape=(n_cols,)))\n",
    "    \n",
    "    # Add addition layers\n",
    "    for i in range(len(numbers)-1):\n",
    "        model.add(Dense(numbers[i+1], activation=activation))\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    # Define early_stopping_monitor\n",
    "    early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, validation_split=0.05, epochs=30, batch_size=batch_size, callbacks=[early_stopping_monitor])\n",
    "\n",
    "    # Get score for predictions\n",
    "    #score = model.evaluate(X_check, y_check)\n",
    "    \n",
    "    # Get root mean squared error\n",
    "    #rmse = np.sqrt(score)\n",
    "    \n",
    "    # Return root mean squared error\n",
    "    #print(rmse)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    submit_to_kaggle(y_pred)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 133295 samples, validate on 7016 samples\n",
      "Epoch 1/30\n",
      "133295/133295 [==============================] - 6s 42us/step - loss: 18.4612 - val_loss: 12.4270\n",
      "Epoch 2/30\n",
      "133295/133295 [==============================] - 5s 39us/step - loss: 13.2583 - val_loss: 11.3641\n",
      "Epoch 3/30\n",
      "133295/133295 [==============================] - 5s 39us/step - loss: 12.9038 - val_loss: 12.8167\n",
      "Epoch 4/30\n",
      "133295/133295 [==============================] - 6s 43us/step - loss: 12.6240 - val_loss: 11.2939\n",
      "Epoch 5/30\n",
      "133295/133295 [==============================] - 5s 39us/step - loss: 12.3720 - val_loss: 11.4262\n",
      "Epoch 6/30\n",
      "133295/133295 [==============================] - 5s 40us/step - loss: 12.1844 - val_loss: 11.2615\n",
      "Epoch 7/30\n",
      "133295/133295 [==============================] - 5s 40us/step - loss: 12.0647 - val_loss: 12.0369\n",
      "Epoch 8/30\n",
      "133295/133295 [==============================] - 5s 41us/step - loss: 11.8806 - val_loss: 10.8719\n",
      "Epoch 9/30\n",
      "133295/133295 [==============================] - 6s 43us/step - loss: 11.8139 - val_loss: 11.2392\n",
      "Epoch 10/30\n",
      "133295/133295 [==============================] - 5s 39us/step - loss: 11.7204 - val_loss: 12.2248\n",
      "                                key  fare_amount\n",
      "0       2015-01-27 13:08:24.0000002    11.411616\n",
      "1       2015-01-27 13:08:24.0000003    12.047842\n",
      "2       2011-10-08 11:53:44.0000002     5.901124\n",
      "3       2012-12-01 21:12:12.0000002     8.974964\n",
      "4       2012-12-01 21:12:12.0000003    16.103685\n",
      "5       2012-12-01 21:12:12.0000005    11.716216\n",
      "6       2011-10-06 12:10:20.0000001     6.982669\n",
      "7       2011-10-06 12:10:20.0000003    46.138313\n",
      "8       2011-10-06 12:10:20.0000002    13.530254\n",
      "9       2014-02-18 15:22:20.0000002     8.573658\n",
      "10      2014-02-18 15:22:20.0000003    11.253660\n",
      "11      2014-02-18 15:22:20.0000001    18.019201\n",
      "12      2010-03-29 20:20:32.0000002     5.905143\n",
      "13      2010-03-29 20:20:32.0000001     7.474408\n",
      "14      2011-10-06 03:59:12.0000002     8.741929\n",
      "15      2011-10-06 03:59:12.0000001    13.148069\n",
      "16      2012-07-15 16:45:04.0000006     5.054348\n",
      "17      2012-07-15 16:45:04.0000002     8.980658\n",
      "18      2012-07-15 16:45:04.0000003     5.200967\n",
      "19      2012-07-15 16:45:04.0000004     4.862925\n",
      "20      2014-10-29 02:09:56.0000001     7.203156\n",
      "21     2014-06-14 13:39:00.00000010     9.399384\n",
      "22     2014-06-14 13:39:00.00000060     7.335324\n",
      "23     2014-06-14 13:39:00.00000087     9.097980\n",
      "24     2014-06-14 13:39:00.00000050    17.585468\n",
      "25      2014-06-14 13:39:00.0000003     7.502512\n",
      "26    2014-06-14 13:39:00.000000158    35.154766\n",
      "27     2014-06-14 13:39:00.00000015    22.675121\n",
      "28     2014-06-14 13:39:00.00000073     6.428345\n",
      "29     2014-06-14 13:39:00.00000077    16.341673\n",
      "...                             ...          ...\n",
      "9884   2013-09-25 22:00:00.00000060    29.814091\n",
      "9885  2013-09-25 22:00:00.000000213    13.950663\n",
      "9886  2013-09-25 22:00:00.000000150    18.381945\n",
      "9887   2013-09-25 22:00:00.00000010     7.956496\n",
      "9888  2013-09-25 22:00:00.000000146     4.313450\n",
      "9889   2013-09-25 22:00:00.00000041     8.462008\n",
      "9890  2013-09-25 22:00:00.000000109    10.117992\n",
      "9891  2013-09-25 22:00:00.000000210    15.351253\n",
      "9892  2013-09-25 22:00:00.000000151     9.603630\n",
      "9893  2013-09-25 22:00:00.000000190    14.072014\n",
      "9894  2013-09-25 22:00:00.000000153     9.953840\n",
      "9895  2013-09-25 22:00:00.000000241    21.717299\n",
      "9896  2013-09-25 22:00:00.000000127     9.555254\n",
      "9897    2015-02-20 11:08:29.0000001    18.409315\n",
      "9898    2015-01-12 15:36:37.0000002     7.559838\n",
      "9899    2015-06-07 00:38:14.0000002    19.040882\n",
      "9900    2015-04-12 21:56:22.0000005     8.097807\n",
      "9901    2015-04-10 11:56:54.0000004     9.590643\n",
      "9902    2015-06-25 01:01:46.0000002    12.706112\n",
      "9903    2015-05-29 10:02:42.0000001    11.925578\n",
      "9904    2015-06-30 20:03:50.0000002    42.206680\n",
      "9905    2015-02-27 19:36:02.0000006    28.779182\n",
      "9906    2015-06-15 01:00:06.0000002     5.671968\n",
      "9907    2015-02-03 09:00:58.0000001    37.010944\n",
      "9908    2015-05-19 13:58:11.0000001    10.024002\n",
      "9909    2015-05-10 12:37:51.0000002    10.266861\n",
      "9910    2015-01-12 17:05:51.0000001    13.247857\n",
      "9911    2015-04-19 20:44:15.0000001    46.258820\n",
      "9912    2015-01-31 01:05:19.0000005    22.629765\n",
      "9913    2015-01-18 14:06:23.0000006     8.562578\n",
      "\n",
      "[9914 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "keras_model = keras_regression_test(X_train, y_train, X_test, numbers=[150, 150, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_tuner(X,y):\n",
    "        \n",
    "    param_grid = [\n",
    "        {'n_estimators': [50, 60, 70], 'max_features': [9, 10, 11]}, \n",
    "    ]\n",
    "    \n",
    "    forest_reg = RandomForestRegressor()\n",
    "    \n",
    "    forest_reg_tuned = GridSearchCV(forest_reg, param_grid, cv=3, \n",
    "                                    scoring='neg_mean_squared_error')\n",
    "    \n",
    "    forest_reg_tuned.fit(X,y)\n",
    "    \n",
    "    # Print the tuned parameters and score\n",
    "    print(\"Tuned Random Forest Parameters: {}\".format(forest_reg_tuned.best_params_))\n",
    "    \n",
    "    scores = cross_val_score(forest_reg_tuned, X, y, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    display_scores('Random Forest', scores)\n",
    "    \n",
    "    return forest_reg_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(title, scores):\n",
    "    rmse = np.sqrt(-scores)\n",
    "    print(title, ' rmse scores:', rmse)\n",
    "    print(title, ' mean score:', rmse.mean())\n",
    "    print(title, ' std:', rmse.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X, y, X_test):\n",
    "    \n",
    "    forest_reg = RandomForestRegressor(max_features=10, n_estimators=40)\n",
    "    \n",
    "    forest_reg.fit(X,y)\n",
    "    \n",
    "    scores = cross_val_score(forest_reg, X, y, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    display_scores('Random Forest', scores)\n",
    "    \n",
    "    y_pred = forest_reg.predict(X_test)\n",
    "    submit_to_kaggle(y_pred)\n",
    "        \n",
    "    return forest_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg = random_forest(X_train, y_train, X_test)\n",
    "feature_importances = forest_reg.feature_importances_\n",
    "sorted(zip(feature_importances, list(X_train)), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda\n",
    "# p42xlarge\n",
    "# ec2 instance pricing\n",
    "# make sure you have gpu and optimization\n",
    "# save as pickle file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
