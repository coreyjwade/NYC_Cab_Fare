{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Cab Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents my solutions to the \"New York City Taxi Fare\" Kaggle competition hosted by Google in 2018. I used a variety of preprocessing techniques and several ML Methods including Random Forests, LightGBM, and Sequential Neural Nets. \n",
    "\n",
    "A fundamental difference between industry and competition became evident during this process. There was an issue with cab rides that started and ended in the same location. Presumably, people go somewhere in a cab, like a grocery store, and return to the same location. If predicting cab fare, it would make sense to input the destination, and double the amount, or sum two round trips. But in this dataset, the goal is to predict fare based on only the timestamp, and the starting and ending coordinates. Since these fares travel no distance according to the dataset, I initially eliminated them during wrangling.\n",
    "\n",
    "To my surpise, these tricky fares remained in the Kaggle competition. So I had to place them back in the data and account for them by segmenting the data. I tried a few approaches like using the mean fare, and averaging it with other models, but they continued to cause problems.\n",
    "\n",
    "Any realistic taxi fare prediction methods would eliminate these fares. Upon elimination, my RMSE was approximately 2.70. Without, it was approximately 3.25. I placed in the top third of the competition, though I could have performed better with a little more preprocessing, and a better cloud connection to utilize deep learning on all 50+ million rows.\n",
    "\n",
    "The pipeline and tests presented below remain as they were on the last day of the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC_Pipeline_Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 2875000\n",
    "NODES = [100,100,50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import model_from_json\n",
    "from sklearn.externals import joblib\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_dataFrame(file_name, subset=True, nrows=ROWS):\n",
    "    if subset:\n",
    "        df = pd.read_csv(file_name, nrows=nrows, parse_dates=['pickup_datetime'])\n",
    "    else:\n",
    "        df = pd.read_csv(file_name, parse_dates=['pickup_datetime'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xtest_ytest(df, split=False):\n",
    "    y_test = df['key']\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    X_test = df.drop('key', axis=1)\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    x = len(df)\n",
    "    print('Length of df:', x)\n",
    "    df = df.dropna(axis=0, subset=['dropoff_latitude'])\n",
    "    df = df.drop('key', axis=1)\n",
    "    y = len(df)\n",
    "    print('NaN dropped:', x-y)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_US(df):\n",
    "    x = len(df)\n",
    "    # Choose cab rides whose pickup and dropoff are the US Mainland\n",
    "    # Declare constants\n",
    "    latmin = 5.496100\n",
    "    latmax = 71.538800\n",
    "    longmin = -124.482003\n",
    "    longmax = -66.885417\n",
    "\n",
    "    # Create dataframe with correct coordinates\n",
    "    df = df[((((df['pickup_longitude']<=longmax) & (df['pickup_longitude']>=longmin)) & ((df['pickup_latitude']<=latmax) & (df['pickup_latitude']>=latmin)))) & ((((df['dropoff_longitude']<=longmax) & (df['dropoff_longitude']>=longmin)) & ((df['dropoff_latitude']<=latmax) & (df['dropoff_latitude']>=latmin))))]\n",
    "    \n",
    "    print('US Mainland Only dropped:', x-len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_NYC(df):\n",
    "    x = len(df)\n",
    "    # Find cab rides whose pickup or dropoff are within NYC boundaries\n",
    "    # Declare constants\n",
    "    latmin = 40.477399\n",
    "    latmax = 40.917577\n",
    "    longmin = -74.259090\n",
    "    longmax = -73.700272\n",
    "\n",
    "    # Create dataframe with correct coordinates\n",
    "    df = df[((((df['pickup_longitude']<=longmax) & (df['pickup_longitude']>=longmin)) & ((df['pickup_latitude']<=latmax) & (df['pickup_latitude']>=latmin)))) | ((((df['dropoff_longitude']<=longmax) % (df['dropoff_longitude']>=longmin)) & ((df['dropoff_latitude']<=latmax) & (df['dropoff_latitude']>=latmin))))]\n",
    "    \n",
    "    print('NYC Taxis Only dropped:', x-len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_Riders(df, num=6):\n",
    "    x = len(df)\n",
    "    # Only choose cabs between 1 and num riders\n",
    "    df = df[(df['passenger_count'] <= num) & (df['passenger_count'] > 0)]\n",
    "    print('Max Passengers 6 dropped:',  x-len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import vincenty\n",
    "\n",
    "def add_distance(df):\n",
    "\n",
    "    # Define coordinates (x,y)\n",
    "    y1 = df['pickup_latitude']\n",
    "    x1 = df['pickup_longitude']\n",
    "    y2 = df['dropoff_latitude']\n",
    "    x2 = df['dropoff_longitude']\n",
    "    \n",
    "    #df['vincenty_distance'] = df.apply(lambda x: vincenty((x['pickup_longitude'], x['pickup_latitude']), (x['dropoff_longitude'], x['dropoff_latitude'])).miles, axis = 1)\n",
    "    \n",
    "    # Create Euclidean Distrance column\n",
    "    df['euclidean_distance'] = np.sqrt((y2-y1)**2 + (x2-x1)**2)\n",
    "\n",
    "    #Create Taxicab Distance column\n",
    "    #df['taxicab_distance'] = np.abs(y2-y1) + np.abs(x2-x1)\n",
    "\n",
    "    # Convert to miles\n",
    "    df['euclidean_distance'] = df['euclidean_distance'] * 69\n",
    "    #df['taxicab_distance'] = df['taxicab_distance'] * 69\n",
    "    \n",
    "    print('Distance Columns added...')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried Vincenty, Euclidean and Taxicab Distances. Interestingly, Euclidean gave the best results. (Note that Vincenty is considered most accurate.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_Fare(df):\n",
    "    # Eliminate unrealistic plots\n",
    "    #df = df[df['fare_amount'] >= (df['vincenty_distance'] * 2 + 2.5)]\n",
    "    df = df[df['fare_amount'] >= (df['euclidean_distance'] * 2 + 2.5)]\n",
    "\n",
    "    print('Min fares dropped:', len(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The min fare rates are given here http://nymag.com/nymetro/urban/features/taxi/n_20286/. The 2.50 base charge is confirmed by histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_Fare(df):\n",
    "    #df = df[(df['fare_amount'] <= (df['vincenty_distance'] * 48 + 16)) | (df['fare_amount'] <= 56)]\n",
    "    df = df[(df['fare_amount'] <= (df['euclidean_distance'] * 48 + 16)) | (df['fare_amount'] <= 56)]\n",
    "\n",
    "    print('Max fares dropped:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some fares are way too high. The max_fare is my attempt at eliminating unrealistic fares. They would entail people sitting in taxis and going literally nowhere for an hour. (Data wrangling revealed unrealistic cab fares of tens of thousands of dollars. Although someone could keep one running for days, it's not helpful for data analysis.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_distance(df):\n",
    "    # Elminate fares that traveled no distance\n",
    "    #df = df[df['vincenty_distance']>0]\n",
    "    df = df[df['euclidean_distance']>0]\n",
    "\n",
    "    print('No distance dropped:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_cap(df, cap=75):\n",
    "    df = df[df['vincenty_distance'] < cap]\n",
    "    print('Distance cap dropped:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_elimination(df):\n",
    "    df = clean_data(df)\n",
    "    df = lat_lon_US(df)\n",
    "    df = lat_lon_NYC(df)\n",
    "    df = max_Riders(df)\n",
    "    df = add_distance(df)\n",
    "    #df = min_Fare(df)\n",
    "    #df = max_Fare(df)\n",
    "    #df = no_distance(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preprocessing is commented out because of the no_distance problem in the kaggle dataset. I had to segment the data and apply these functions later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_train, y_train Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_y(df, split=False):\n",
    "    X = df.drop('fare_amount', axis=1)\n",
    "    y = df['fare_amount'].copy()\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garbage Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of accumulated garbage\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWESOME trick. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Time_units(df):\n",
    "    \n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['minute'] = df['pickup_datetime'].dt.minute\n",
    "    df['second'] = df['pickup_datetime'].dt.second\n",
    "    df['dayofweek'] = df['pickup_datetime'].dt.dayofweek\n",
    "    \n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "    dr = pd.date_range(start='2009-01-01', end='2015-12-31')\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "    df['holiday'] = df['pickup_datetime'].dt.date.astype('datetime64').isin(holidays)\n",
    "    \n",
    "    df = df.drop('pickup_datetime', axis=1)\n",
    "\n",
    "    df['total_seconds'] = 3600 * df['hour'] + 60 * df['minute'] + df['second']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Time_columns(df):\n",
    "    \n",
    "    def morning_rush(row):\n",
    "        if ((row['hour'] in [6,7,8,9]) & (row['dayofweek'] in [0,1,2,3,4])) & (not row['holiday']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['morning_rush'] = df.apply(morning_rush, axis=1)\n",
    "\n",
    "    def night_charge(row):\n",
    "        if row['hour'] in [20,21,22,23,24,1,2,3,4,5,6]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['night_charge'] = df.apply(night_charge, axis=1)\n",
    "\n",
    "    def weekday_surcharge(row):\n",
    "        if ((row['hour'] in [16,17,18,19,20]) & (row['dayofweek'] in [0,1,2,3,4])) & (not row['holiday']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['weekday_surcharge'] = df.apply(weekday_surcharge, axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Time(df):\n",
    "    df = add_Time_units(df)\n",
    "    df = add_Time_columns(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define line from two points and a provided column\n",
    "def two_points_line(a, b, column):\n",
    "        \n",
    "    # Case when y-values are the same\n",
    "    if b[1]==a[1]:\n",
    "        \n",
    "        # Slope defaults to 0\n",
    "        slope = 0\n",
    "        \n",
    "    # Case when x-values are the same\n",
    "    elif b[0]==a[0]:\n",
    "        \n",
    "        # Case when max value is less than 999999999\n",
    "        if column.max() < 999999999:\n",
    "            \n",
    "            # Add 999999999 to max value\n",
    "            slope = column.max() + 999999999\n",
    "        \n",
    "        # All other cases\n",
    "        else:\n",
    "            \n",
    "            # Multiply max value by itself (greater than 999999999)\n",
    "            slope = column.max() * column.max()\n",
    "    \n",
    "    # When x-values and y-values are not 0\n",
    "    else:\n",
    "        \n",
    "        # Use standard slope formula\n",
    "        slope = (b[1] - a[1])/(b[0]-a[0])\n",
    "    \n",
    "    \n",
    "    # Equation for y-intercept (solving y=mx+b for b)\n",
    "    y_int = a[1] - slope * a[0]\n",
    "    \n",
    "    # Return slope and y-intercept\n",
    "    return slope, y_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_cols(df):\n",
    "    \n",
    "    upper_right = (-73.929224, 40.804328)\n",
    "    bottom_right = (-73.980036, 40.710706)\n",
    "    bottom_left = (-74.054880, 40.681292)\n",
    "    upper_left = (-73.966303, 40.830050)\n",
    "\n",
    "    m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "    m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "    m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "    m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "    def manhattan_pickup(row):\n",
    "        if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "        (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['pickup_latitude'] >= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "        (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    df['manhattan_pickup'] = df.apply(manhattan_pickup, axis=1)\n",
    "    \n",
    "    \n",
    "    def manhattan_dropoff(row):\n",
    "        if (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "        (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "        (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    df['manhattan_dropoff'] = df.apply(manhattan_dropoff, axis=1)\n",
    "    \n",
    "    \n",
    "    def manhattan(row):\n",
    "        if (row['manhattan_pickup']) & (row['manhattan_dropoff']):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    df['manhattan'] = df.apply(manhattan, axis=1)\n",
    "    \n",
    "    \n",
    "    def manhattan_one_way(row):\n",
    "        if (not row['manhattan']) & (row['manhattan_pickup']) | (row['manhattan_dropoff']):\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "    df['manhattan_one_way'] = df.apply(manhattan_one_way, axis=1)\n",
    "     \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the best method, I eyeballed 4 points on a digital map and drew a quadrilateral around them to bound Manhattan. Since this method was relatively time-consuming, I only applied it to a couple other geographic areas with known surchages, trips to Newark and the JFK Airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newark_cols(df):\n",
    "    \n",
    "    upper_right = (-74.107867, 40.718282)\n",
    "    bottom_right = (-74.143665, 40.654673)\n",
    "    bottom_left = (-74.250524, 40.698436)\n",
    "    upper_left = (-74.171983, 40.792347)\n",
    "\n",
    "    m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "    m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "    m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "    m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "    def newark(row):\n",
    "        if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "        (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['pickup_latitude'] >= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "        (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))) | (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "        (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "        (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    df['newark'] = df.apply(newark, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jkf_cols(df):\n",
    "    \n",
    "    upper_right = (-73.789700, 40.663781)\n",
    "    bottom_right = (-73.762112, 40.633567)\n",
    "    bottom_left = (-73.818920, 40.642250)\n",
    "    upper_left = (-73.804656, 40.664858)\n",
    "\n",
    "    m_top, b_top = two_points_line(upper_right, upper_left, df.pickup_latitude)\n",
    "    m_left, b_left = two_points_line(bottom_left, upper_left, df.pickup_latitude)\n",
    "    m_right, b_right = two_points_line(bottom_right, upper_right, df.pickup_latitude)\n",
    "    m_bottom, b_bottom = two_points_line(bottom_right, bottom_left, df.pickup_latitude)\n",
    "\n",
    "    def jfk(row):\n",
    "        if (((row['pickup_latitude'] <= (row['pickup_longitude'] * m_top + b_top)) &\n",
    "        (row['pickup_latitude'] >= (row['pickup_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['pickup_latitude'] <= (row['pickup_longitude'] * m_right + b_right)) &\n",
    "        (row['pickup_latitude'] <= (row['pickup_longitude'] * m_left + b_left)))) | (((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_top + b_top)) &\n",
    "        (row['dropoff_latitude'] >= (row['dropoff_longitude'] * m_bottom + b_bottom))) &\n",
    "        ((row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_right + b_right)) &\n",
    "        (row['dropoff_latitude'] <= (row['dropoff_longitude'] * m_left + b_left)))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    df['jfk'] = df.apply(jfk, axis=1)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_locations(df):\n",
    "    df = manhattan_cols(df)\n",
    "    df = jkf_cols(df)\n",
    "    df = newark_cols(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cols(df):\n",
    "    df = add_Time(df)\n",
    "    df = add_locations(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_predictor_cols(df, no_dist=False): \n",
    "    if no_dist:\n",
    "        cols=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'year', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'total_seconds', 'morning_rush', 'night_charge', 'weekday_surcharge', 'manhattan', 'manhattan_one_way', 'jfk', 'newark', 'passenger_count', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "    else: \n",
    "        cols=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'year', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'total_seconds', 'morning_rush', 'night_charge', 'weekday_surcharge', 'manhattan', 'manhattan_one_way', 'jfk', 'newark', 'passenger_count','euclidean_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "    X = df[cols]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_cols(X):\n",
    "    X = one_Hot_Encoder(X, X['month'])\n",
    "    del X['month']\n",
    "    X = one_Hot_Encoder(X, X['dayofweek'], month=False)\n",
    "    del X['dayofweek']\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_Hot_Encoder(X, col, month=True): \n",
    "    encoder = OneHotEncoder()\n",
    "    hot_array = encoder.fit_transform(np.array(col).reshape(-1,1)).toarray()\n",
    "    hot_df = pd.DataFrame(hot_array)\n",
    "    if month:\n",
    "        hot_df.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    else:\n",
    "        hot_df.columns = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "    new_df = X.join(hot_df)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_split(X_train, y_train):\n",
    "            \n",
    "    y = y_train.median()\n",
    "    mse = np.sum((y_train-y)**2)\n",
    "    score = mse/len(y_train)\n",
    "    rmse = np.sqrt(score)\n",
    "    print('Lin reg train rmse:', rmse)\n",
    "    print('Lin reg train mean:', rmse.mean())\n",
    "    print('Lin reg train std:', rmse.std())\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X_train, y_train, distance_none=False, distance_high=False):\n",
    "        \n",
    "    print('Length of X:', len(X_train))\n",
    "    lr_model = LinearRegression(fit_intercept=False)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    scores = cross_val_score(lr_model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse = np.sqrt(-scores)\n",
    "    print('Lin reg train rmse:', rmse)\n",
    "    print('Lin reg train mean:', rmse.mean())\n",
    "    print('Lin reg train std:', rmse.std())\n",
    "    \n",
    "    if distance_none:\n",
    "        joblib.dump(lr_model, 'lr_distance_none_model.pkl')\n",
    "        print('Linear Regression model saved as \"lr_distance_none_model.pkl\"')\n",
    "    elif distance_high:\n",
    "        joblib.dump(lr_model, 'lr_distance_high_model.pkl')\n",
    "        print('Linear Regression model saved as \"lr_distance_high_model.pkl\"')\n",
    "    else:\n",
    "        joblib.dump(lr_model, 'lr_model.pkl') \n",
    "        print('Linear Regression model saved as \"lr_model.pkl\"')\n",
    "\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(X_train, y_train, distance_none=False, distance_high=False):\n",
    "        \n",
    "    print('Length of X:', len(X_train))\n",
    "    ri_model = Ridge()\n",
    "    ri_model.fit(X_train, y_train)\n",
    "    scores = cross_val_score(ri_model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse = np.sqrt(-scores)\n",
    "    print('Lin reg train rmse:', rmse)\n",
    "    print('Lin reg train mean:', rmse.mean())\n",
    "    print('Lin reg train std:', rmse.std())\n",
    "    \n",
    "    if distance_none:\n",
    "        joblib.dump(ri_model, 'ri_distance_none_model.pkl')\n",
    "        print('Linear Regression model saved as \"ri_distance_none_model.pkl\"')\n",
    "    elif distance_high:\n",
    "        joblib.dump(ri_model, 'ri_distance_high_model.pkl')\n",
    "        print('Linear Regression model saved as \"ri_distance_high_model.pkl\"')\n",
    "    else:\n",
    "        joblib.dump(ri_model, 'ri_model.pkl') \n",
    "        print('Linear Regression model saved as \"ri_model.pkl\"')\n",
    "\n",
    "    return ri_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_random_forest_tuner(X_train, y_train):\n",
    "        \n",
    "    param_grid = [\n",
    "        {'max_features': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9], \n",
    "         'min_samples_leaf': [3, 5, 7, 9],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "        }, \n",
    "    ]\n",
    "    \n",
    "    forest_reg = RandomForestRegressor(n_jobs=-1)\n",
    "    \n",
    "    forest_reg_tuned = RandomizedSearchCV(forest_reg, param_grid, cv=3, n_iter=10, \n",
    "                                    scoring='neg_mean_squared_error')\n",
    "    \n",
    "    forest_reg_tuned.fit(X,y)\n",
    "    \n",
    "    # Print the tuned parameters and score\n",
    "    print(\"Tuned Random Forest Parameters: {}\".format(forest_reg_tuned.best_params_))\n",
    "    \n",
    "    return forest_reg_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(title, scores):\n",
    "    rmse = np.sqrt(-scores)\n",
    "    print(title, ' rmse scores:', rmse)\n",
    "    print(title, ' mean score:', rmse.mean())\n",
    "    print(title, ' std:', rmse.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, distance_none=False, distance_high=False):\n",
    "    \n",
    "    rf_model = RandomForestRegressor(max_features=16, n_estimators=500, min_samples_leaf=3, min_samples_split=12, max_depth=None, n_jobs=-1)\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    scores = cross_val_score(rf_model, X_train, y_train, scoring='neg_mean_squared_error', cv=2)\n",
    "    \n",
    "    display_scores('Random Forest', scores)\n",
    "    \n",
    "    if distance_none:\n",
    "        joblib.dump(rf_model, 'rf_distance_none_500_model.pkl')\n",
    "        print('Random Forest model saved as \"rf_distance_none_500_model.pkl\"')\n",
    "    else:\n",
    "        #joblib.dump(rf_model, 'rf_model.pkl') \n",
    "        #print('Random Forest model saved as \"rf_model.pkl\"')\n",
    "        joblib.dump(rf_model, 'rf_sum_model.pkl') \n",
    "        print('Random Forest model saved as \"rf_sum_model.pkl\"')\n",
    "        \n",
    "    return rf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_regression_test requires \"from sklearn.model_selection import train_test_split\"\n",
    "def deep_learning(X_train, y_train, nodes=NODES, batch_size=32, activation='relu', optimizer='adam', loss='mean_squared_error', keras_distance_high=False, keras_distance_none=False):\n",
    "        \n",
    "    X, X_check, y, y_check = train_test_split(X_train, y_train, test_size=0.05)\n",
    "    \n",
    "    # Save the number of columns in predictors: n_cols\n",
    "    n_cols = X.shape[1]\n",
    "\n",
    "    # Set up the model: model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add the first layer\n",
    "    model.add(Dense(nodes[0], activation=activation, input_shape=(n_cols,)))\n",
    "    \n",
    "    # Add addition layers\n",
    "    for i in range(len(nodes)-1):\n",
    "        model.add(Dense(nodes[i+1], activation=activation, kernel_constraint=maxnorm(3)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    # Define early_stopping_monitor\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X, y, validation_split=0.05, epochs=1000, batch_size=batch_size, callbacks=[early_stopping_monitor])\n",
    "\n",
    "    # Get score for predictions\n",
    "    score = model.evaluate(X_check, y_check)\n",
    "    \n",
    "    # Get root mean squared error\n",
    "    rmse = np.sqrt(score)\n",
    "    \n",
    "    # Return root mean squared error\n",
    "    print(rmse)\n",
    "    \n",
    "    save_keras_model(model, keras_distance_high=keras_distance_high, keras_distance_none=keras_distance_none)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(X):\n",
    "    X = X.reset_index(drop=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_frame_split(df):\n",
    "    \n",
    "    #df_distance_none = df[df['vincenty_distance']==0]\n",
    "    df_distance_none = df[df['euclidean_distance']==0]\n",
    "\n",
    "    print('New dataframe \"df_distance_none\" created with length:', len(df_distance_none))\n",
    "    \n",
    "    #df_distance_high = df[df['vincenty_distance']>30]\n",
    "    df_distance_high = df[df['euclidean_distance']>30]\n",
    "\n",
    "    print('New dataframe \"df_distance_high\" created with length:', len(df_distance_high))\n",
    "\n",
    "    #df = df[df['vincenty_distance']>0]\n",
    "    #df = df[df['vincenty_distance']<=30]\n",
    "    \n",
    "    df = df[df['euclidean_distance']>0]\n",
    "    df = df[df['euclidean_distance']<=30]\n",
    "\n",
    "    print('New length of original dataframe:', len(df))\n",
    "    return df, df_distance_none, df_distance_high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to split the data to improve results. If there were no Kaggle competition with unhelpful data, df_distance_none and df_distance_high would not exist. The test points that traveled far distances posed problems, presumably because they did not have to deal with New York traffic. I tried this idea late in the game, and had minimal improvements at best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_pipeline(df, no_dist=False):\n",
    "    df = reset_index(df)\n",
    "    df = add_cols(df)\n",
    "    df = one_hot_cols(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_pipeline(X, no_dist=False):\n",
    "    X = choose_predictor_cols(X, no_dist=no_dist)\n",
    "    X = standard_scaler(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline(test_set=False, max_scaler=True):\n",
    "    df = file_to_dataFrame('test.csv')\n",
    "    print('Length of test_df:)', len(df))\n",
    "    df = add_distance(df)\n",
    "    df = df_pipeline(df)\n",
    "    df, df_distance_none, df_distance_high = data_frame_split(df)\n",
    "    \n",
    "    X_test, y_test = make_Xtest_ytest(df)\n",
    "    X_test_distance_none, y_test_distance_none = make_Xtest_ytest(df_distance_none)\n",
    "    X_test_distance_high, y_test_distance_high = make_Xtest_ytest(df_distance_high)\n",
    "    \n",
    "    X_test = X_pipeline(df)\n",
    "    X_test_distance_none = X_pipeline(X_test_distance_none, no_dist=True)\n",
    "    X_test_distance_high = X_pipeline(X_test_distance_high)\n",
    "    \n",
    "    return X_test, y_test, X_test_distance_none, y_test_distance_none, X_test_distance_high, y_test_distance_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "    \n",
    "    df = file_to_dataFrame('train.csv')\n",
    "    df = row_elimination(df)\n",
    "    df = df_pipeline(df)\n",
    "    df, df_distance_none, df_distance_high = data_frame_split(df)\n",
    "    df = min_Fare(df)\n",
    "    df = max_Fare(df)\n",
    "    \n",
    "    X, y = make_X_y(df)\n",
    "    X_distance_none, y_distance_none = make_X_y(df_distance_none)\n",
    "    X_distance_high, y_distance_high = make_X_y(df_distance_high)\n",
    "    \n",
    "    X = X_pipeline(X)\n",
    "    X_distance_none = X_pipeline(X_distance_none, no_dist=True)\n",
    "    X_distance_high = X_pipeline(X_distance_high)\n",
    "    \n",
    "    return X, y, X_distance_none, y_distance_none, X_distance_high, y_distance_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keras_model(model, keras_distance_none=False, keras_distance_high=False):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    if keras_distance_none:\n",
    "        with open(\"dl_distance_none_model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"dl_distance_none_model.h5\")\n",
    "        print(\"Saved deep learning model as 'dl_distance_none_model.json'\")\n",
    "    \n",
    "    elif keras_distance_high:\n",
    "        with open(\"dl_distance_high_model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"dl_distance_high_model.h5\")\n",
    "        print(\"Saved deep learning model as 'dl_distance_high_model.json'\")\n",
    "    \n",
    "    else:\n",
    "        with open(\"dl_model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"dl_model.h5\")\n",
    "        print(\"Saved deep learning model as 'dl_model.json'\")\n",
    "    return model\n",
    "  \n",
    "def open_keras_model(file, keras_distance_none=False, keras_distance_high=False):\n",
    "    # load json and create model\n",
    "    json_file = open(file, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    if keras_distance_none:\n",
    "        loaded_model.load_weights(\"dl_distance_none_model.h5\")\n",
    "    elif keras_distance_high:\n",
    "        loaded_model.load_weights(\"dl_distance_high_model.h5\")\n",
    "    else:\n",
    "        loaded_model.load_weights(\"dl_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fare = y.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_val(row):\n",
    "    if row['fare_amount'] < 2.5:\n",
    "        return 2.5\n",
    "    else:\n",
    "        return row['fare_amount']\n",
    "\n",
    "def open_model(saved_model, keras=False, keras_distance_none=False, keras_distance_high=False):\n",
    "    if keras:\n",
    "        model = open_keras_model(saved_model)\n",
    "    elif keras_distance_none:\n",
    "        model = open_keras_model(saved_model, keras_distance_none=keras_distance_none)\n",
    "    elif keras_distance_high:\n",
    "        model = open_keras_model(saved_model, keras_distance_high=keras_distance_high)\n",
    "    else:\n",
    "        model = joblib.load(saved_model)\n",
    "    return model\n",
    "\n",
    "def kaggle_submit(y_test, saved_model, saved_model_distance_none, saved_model_distance_high, keras=False, keras_distance_none=False, keras_distance_high=False):\n",
    "    saved_model = open_model(saved_model, keras=keras)\n",
    "    saved_model_distance_none = open_model(saved_model_distance_none, keras_distance_none=keras_distance_none)\n",
    "    saved_model_distance_high = open_model(saved_model_distance_high, keras_distance_high=keras_distance_high)\n",
    "        \n",
    "    y_test['fare_amount'] = saved_model.predict(X_test)\n",
    "    \n",
    "    y_est = 0.775*(saved_model_distance_none.predict(X_test_distance_none))\n",
    "    y_mean = 0.225*(mean_fare)    \n",
    "    y_test_distance_none['fare_amount'] = y_est + y_mean\n",
    "    \n",
    "    y_test_distance_high['fare_amount'] = saved_model_distance_high.predict(X_test_distance_high)\n",
    "    \n",
    "    y_test = pd.concat([y_test,y_test_distance_none,y_test_distance_high])\n",
    "        \n",
    "    y_test['fare_amount'] = y_test.apply(min_val, axis=1)\n",
    "    \n",
    "    y_test.to_csv('my_submission.csv', index=False)\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of df: 2875000\n",
      "NaN dropped: 23\n",
      "US Mainland Only dropped: 59423\n",
      "NYC Taxis Only dropped: 3079\n",
      "Max Passengers 6 dropped: 9912\n",
      "Distance Columns added...\n",
      "New dataframe \"df_distance_none\" created with length: 29021\n",
      "New dataframe \"df_distance_high\" created with length: 509\n",
      "New length of original dataframe: 2773033\n",
      "Min fares dropped: 2692680\n",
      "Max fares dropped: 2691545\n"
     ]
    }
   ],
   "source": [
    "X, y, X_distance_none, y_distance_none, X_distance_high, y_distance_high = pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_df:) 9914\n",
      "Distance Columns added...\n",
      "New dataframe \"df_distance_none\" created with length: 85\n",
      "New dataframe \"df_distance_high\" created with length: 3\n",
      "New length of original dataframe: 9826\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test, X_test_distance_none, y_test_distance_none, X_test_distance_high, y_test_distance_high = test_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sum = pd.concat([X, X_distance_high])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sum = pd.concat([y, y_distance_high])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 2692054\n",
      "Lin reg train rmse: [3.62689569 3.63029866 3.64863394 3.5993213  4.65208551]\n",
      "Lin reg train mean: 3.831447019811648\n",
      "Lin reg train std: 0.4106220697799635\n",
      "Linear Regression model saved as \"ri_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge(X_sum, y_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 29021\n",
      "Lin reg train rmse: [13.74402807 12.12454211 12.15907035 11.3554581  12.41577521]\n",
      "Lin reg train mean: 12.359774768520234\n",
      "Lin reg train std: 0.7776325483568045\n",
      "Linear Regression model saved as \"ri_distance_none_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge(X_distance_none, y_distance_none, distance_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 509\n",
      "Lin reg train rmse: [51.05656482 45.74069735 50.12550284 67.8563549  53.73753999]\n",
      "Lin reg train mean: 53.703331978952406\n",
      "Lin reg train std: 7.530192773741098\n",
      "Linear Regression model saved as \"ri_distance_high_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge(X_distance_high, y_distance_high, distance_high=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26190 samples, validate on 1379 samples\n",
      "Epoch 1/1000\n",
      "26190/26190 [==============================] - 2s 59us/step - loss: 162.1333 - val_loss: 120.3129\n",
      "Epoch 2/1000\n",
      "26190/26190 [==============================] - 1s 39us/step - loss: 154.5156 - val_loss: 118.2167\n",
      "Epoch 3/1000\n",
      "26190/26190 [==============================] - 1s 40us/step - loss: 153.7974 - val_loss: 120.1640\n",
      "Epoch 4/1000\n",
      "26190/26190 [==============================] - 1s 41us/step - loss: 153.0455 - val_loss: 119.1551\n",
      "Epoch 5/1000\n",
      "26190/26190 [==============================] - 1s 42us/step - loss: 151.7777 - val_loss: 118.0005\n",
      "Epoch 6/1000\n",
      "26190/26190 [==============================] - 1s 42us/step - loss: 151.3069 - val_loss: 117.1391\n",
      "Epoch 7/1000\n",
      "26190/26190 [==============================] - 1s 40us/step - loss: 149.2023 - val_loss: 120.3057\n",
      "Epoch 8/1000\n",
      "26190/26190 [==============================] - 1s 41us/step - loss: 148.0701 - val_loss: 114.9488\n",
      "Epoch 9/1000\n",
      "26190/26190 [==============================] - 1s 42us/step - loss: 147.7094 - val_loss: 121.3408\n",
      "Epoch 10/1000\n",
      "26190/26190 [==============================] - 1s 42us/step - loss: 145.6839 - val_loss: 119.1398\n",
      "Epoch 11/1000\n",
      "26190/26190 [==============================] - 1s 41us/step - loss: 145.5049 - val_loss: 119.2680\n",
      "1452/1452 [==============================] - 0s 15us/step\n",
      "11.161159377587223\n",
      "Saved deep learning model as 'dl_distance_none_model.json'\n"
     ]
    }
   ],
   "source": [
    "dl_model_distance = deep_learning(X_distance_none, y_distance_none, keras_distance_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 458 samples, validate on 25 samples\n",
      "Epoch 1/1000\n",
      "458/458 [==============================] - 1s 1ms/step - loss: 9868.3481 - val_loss: 10804.8379\n",
      "Epoch 2/1000\n",
      "458/458 [==============================] - 0s 45us/step - loss: 9192.0774 - val_loss: 9588.5059\n",
      "Epoch 3/1000\n",
      "458/458 [==============================] - 0s 45us/step - loss: 7697.0159 - val_loss: 7053.6689\n",
      "Epoch 4/1000\n",
      "458/458 [==============================] - 0s 53us/step - loss: 5238.3887 - val_loss: 4018.0220\n",
      "Epoch 5/1000\n",
      "458/458 [==============================] - 0s 52us/step - loss: 3988.3457 - val_loss: 2671.0505\n",
      "Epoch 6/1000\n",
      "458/458 [==============================] - 0s 48us/step - loss: 3611.7167 - val_loss: 2264.7693\n",
      "Epoch 7/1000\n",
      "458/458 [==============================] - 0s 56us/step - loss: 3320.6680 - val_loss: 2230.1462\n",
      "Epoch 8/1000\n",
      "458/458 [==============================] - 0s 50us/step - loss: 3186.4581 - val_loss: 2012.8912\n",
      "Epoch 9/1000\n",
      "458/458 [==============================] - 0s 44us/step - loss: 2923.3905 - val_loss: 1896.2798\n",
      "Epoch 10/1000\n",
      "458/458 [==============================] - 0s 63us/step - loss: 2641.9846 - val_loss: 1881.6611\n",
      "Epoch 11/1000\n",
      "458/458 [==============================] - 0s 48us/step - loss: 2471.3842 - val_loss: 1787.3672\n",
      "Epoch 12/1000\n",
      "458/458 [==============================] - 0s 57us/step - loss: 2505.0273 - val_loss: 1779.5601\n",
      "Epoch 13/1000\n",
      "458/458 [==============================] - 0s 51us/step - loss: 2261.9745 - val_loss: 1842.7015\n",
      "Epoch 14/1000\n",
      "458/458 [==============================] - ETA: 0s - loss: 2327.34 - 0s 58us/step - loss: 2334.5848 - val_loss: 1811.0233\n",
      "Epoch 15/1000\n",
      "458/458 [==============================] - 0s 58us/step - loss: 2326.0015 - val_loss: 1794.0631\n",
      "26/26 [==============================] - 0s 74us/step\n",
      "38.46130646151483\n",
      "Saved deep learning model as 'dl_distance_high_model.json'\n"
     ]
    }
   ],
   "source": [
    "dl_model_distance = deep_learning(X_distance_high, y_distance_high, keras_distance_high=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2429118 samples, validate on 127849 samples\n",
      "Epoch 1/1000\n",
      "2429118/2429118 [==============================] - 107s 44us/step - loss: 12.4999 - val_loss: 9.7576\n",
      "Epoch 2/1000\n",
      "2429118/2429118 [==============================] - 108s 45us/step - loss: 10.7824 - val_loss: 9.6408\n",
      "Epoch 3/1000\n",
      "2429118/2429118 [==============================] - 111s 46us/step - loss: 10.5082 - val_loss: 9.3295\n",
      "Epoch 4/1000\n",
      "2429118/2429118 [==============================] - 100s 41us/step - loss: 10.3397 - val_loss: 9.0330\n",
      "Epoch 5/1000\n",
      "2429118/2429118 [==============================] - 96s 40us/step - loss: 10.2256 - val_loss: 9.4364\n",
      "Epoch 6/1000\n",
      "2429118/2429118 [==============================] - 96s 40us/step - loss: 10.1093 - val_loss: 8.9241\n",
      "Epoch 7/1000\n",
      "2429118/2429118 [==============================] - 97s 40us/step - loss: 9.9892 - val_loss: 8.9832\n",
      "Epoch 8/1000\n",
      "2429118/2429118 [==============================] - 97s 40us/step - loss: 9.9665 - val_loss: 12.1911\n",
      "Epoch 9/1000\n",
      "2429118/2429118 [==============================] - 97s 40us/step - loss: 9.9324 - val_loss: 9.0024\n",
      "134578/134578 [==============================] - 2s 12us/step\n",
      "3.0603275437869937\n",
      "Saved deep learning model as 'dl_model.json'\n"
     ]
    }
   ],
   "source": [
    "dl_model = deep_learning(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-27 13:08:24.0000002</td>\n",
       "      <td>10.025081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-27 13:08:24.0000003</td>\n",
       "      <td>10.261865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-08 11:53:44.0000002</td>\n",
       "      <td>4.292730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-01 21:12:12.0000002</td>\n",
       "      <td>9.042953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-01 21:12:12.0000003</td>\n",
       "      <td>16.544141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012-12-01 21:12:12.0000005</td>\n",
       "      <td>10.167219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2011-10-06 12:10:20.0000001</td>\n",
       "      <td>5.374145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2011-10-06 12:10:20.0000003</td>\n",
       "      <td>48.807804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2011-10-06 12:10:20.0000002</td>\n",
       "      <td>11.000858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2014-02-18 15:22:20.0000002</td>\n",
       "      <td>6.555713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2014-02-18 15:22:20.0000003</td>\n",
       "      <td>9.619018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2014-02-18 15:22:20.0000001</td>\n",
       "      <td>19.362898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2010-03-29 20:20:32.0000002</td>\n",
       "      <td>4.337511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2010-03-29 20:20:32.0000001</td>\n",
       "      <td>6.469858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2011-10-06 03:59:12.0000002</td>\n",
       "      <td>7.540002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2011-10-06 03:59:12.0000001</td>\n",
       "      <td>10.614314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2012-07-15 16:45:04.0000006</td>\n",
       "      <td>6.092904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012-07-15 16:45:04.0000002</td>\n",
       "      <td>8.532851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2012-07-15 16:45:04.0000003</td>\n",
       "      <td>4.957576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2012-07-15 16:45:04.0000004</td>\n",
       "      <td>4.513414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2014-10-29 02:09:56.0000001</td>\n",
       "      <td>7.378500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2014-06-14 13:39:00.00000010</td>\n",
       "      <td>8.789781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2014-06-14 13:39:00.00000060</td>\n",
       "      <td>7.034381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2014-06-14 13:39:00.00000087</td>\n",
       "      <td>8.672409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2014-06-14 13:39:00.00000050</td>\n",
       "      <td>18.288989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2014-06-14 13:39:00.0000003</td>\n",
       "      <td>7.097782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2014-06-14 13:39:00.000000158</td>\n",
       "      <td>39.493371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2014-06-14 13:39:00.00000015</td>\n",
       "      <td>24.924428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2014-06-14 13:39:00.00000073</td>\n",
       "      <td>6.046691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2014-06-14 13:39:00.00000077</td>\n",
       "      <td>16.698577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>2010-08-14 02:13:00.0000003</td>\n",
       "      <td>18.127769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7198</th>\n",
       "      <td>2010-09-29 16:43:00.000000172</td>\n",
       "      <td>9.332487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7322</th>\n",
       "      <td>2011-10-04 09:37:00.00000077</td>\n",
       "      <td>7.654973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7526</th>\n",
       "      <td>2011-03-06 21:01:00.00000081</td>\n",
       "      <td>11.478248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>2012-01-26 07:33:00.000000136</td>\n",
       "      <td>8.819078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>2012-01-26 07:33:00.00000023</td>\n",
       "      <td>8.758911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7637</th>\n",
       "      <td>2010-12-09 07:29:00.00000083</td>\n",
       "      <td>8.853547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7839</th>\n",
       "      <td>2012-12-15 06:35:45.0000001</td>\n",
       "      <td>14.201680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7874</th>\n",
       "      <td>2009-06-02 08:34:33.0000001</td>\n",
       "      <td>8.591836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>2013-01-19 20:19:45.0000002</td>\n",
       "      <td>10.114151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8159</th>\n",
       "      <td>2011-07-20 08:05:02.0000001</td>\n",
       "      <td>9.613750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381</th>\n",
       "      <td>2009-07-30 15:49:15.0000002</td>\n",
       "      <td>18.177299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8424</th>\n",
       "      <td>2009-06-10 16:55:00.000000155</td>\n",
       "      <td>9.628827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8426</th>\n",
       "      <td>2011-06-24 12:03:00.000000131</td>\n",
       "      <td>8.957011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8517</th>\n",
       "      <td>2011-03-11 01:44:00.00000067</td>\n",
       "      <td>9.817891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596</th>\n",
       "      <td>2010-09-20 16:48:00.00000021</td>\n",
       "      <td>8.817124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8632</th>\n",
       "      <td>2009-10-29 10:48:40.0000003</td>\n",
       "      <td>8.977029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8835</th>\n",
       "      <td>2012-12-01 21:12:12.0000001</td>\n",
       "      <td>9.632716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9006</th>\n",
       "      <td>2013-07-02 22:27:14.0000001</td>\n",
       "      <td>11.935193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221</th>\n",
       "      <td>2010-08-27 18:45:00.000000215</td>\n",
       "      <td>8.500797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9383</th>\n",
       "      <td>2011-03-11 01:44:00.00000064</td>\n",
       "      <td>8.998912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9421</th>\n",
       "      <td>2011-10-04 09:37:00.00000088</td>\n",
       "      <td>7.518601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9478</th>\n",
       "      <td>2011-12-13 22:00:00.00000044</td>\n",
       "      <td>12.372449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>2014-07-21 18:19:00.00000025</td>\n",
       "      <td>7.751128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9563</th>\n",
       "      <td>2011-03-06 21:01:00.00000018</td>\n",
       "      <td>8.575669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9830</th>\n",
       "      <td>2014-07-21 18:19:00.00000065</td>\n",
       "      <td>7.517757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9888</th>\n",
       "      <td>2013-09-25 22:00:00.000000146</td>\n",
       "      <td>7.461161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>2010-06-11 13:37:21.0000004</td>\n",
       "      <td>44.258225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5887</th>\n",
       "      <td>2010-07-04 16:44:11.0000002</td>\n",
       "      <td>42.257711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8529</th>\n",
       "      <td>2009-11-25 19:32:52.0000001</td>\n",
       "      <td>25.599010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9914 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                key  fare_amount\n",
       "0       2015-01-27 13:08:24.0000002    10.025081\n",
       "1       2015-01-27 13:08:24.0000003    10.261865\n",
       "2       2011-10-08 11:53:44.0000002     4.292730\n",
       "3       2012-12-01 21:12:12.0000002     9.042953\n",
       "4       2012-12-01 21:12:12.0000003    16.544141\n",
       "5       2012-12-01 21:12:12.0000005    10.167219\n",
       "6       2011-10-06 12:10:20.0000001     5.374145\n",
       "7       2011-10-06 12:10:20.0000003    48.807804\n",
       "8       2011-10-06 12:10:20.0000002    11.000858\n",
       "9       2014-02-18 15:22:20.0000002     6.555713\n",
       "10      2014-02-18 15:22:20.0000003     9.619018\n",
       "11      2014-02-18 15:22:20.0000001    19.362898\n",
       "12      2010-03-29 20:20:32.0000002     4.337511\n",
       "13      2010-03-29 20:20:32.0000001     6.469858\n",
       "14      2011-10-06 03:59:12.0000002     7.540002\n",
       "15      2011-10-06 03:59:12.0000001    10.614314\n",
       "16      2012-07-15 16:45:04.0000006     6.092904\n",
       "17      2012-07-15 16:45:04.0000002     8.532851\n",
       "18      2012-07-15 16:45:04.0000003     4.957576\n",
       "19      2012-07-15 16:45:04.0000004     4.513414\n",
       "20      2014-10-29 02:09:56.0000001     7.378500\n",
       "21     2014-06-14 13:39:00.00000010     8.789781\n",
       "22     2014-06-14 13:39:00.00000060     7.034381\n",
       "23     2014-06-14 13:39:00.00000087     8.672409\n",
       "24     2014-06-14 13:39:00.00000050    18.288989\n",
       "25      2014-06-14 13:39:00.0000003     7.097782\n",
       "26    2014-06-14 13:39:00.000000158    39.493371\n",
       "27     2014-06-14 13:39:00.00000015    24.924428\n",
       "28     2014-06-14 13:39:00.00000073     6.046691\n",
       "29     2014-06-14 13:39:00.00000077    16.698577\n",
       "...                             ...          ...\n",
       "7163    2010-08-14 02:13:00.0000003    18.127769\n",
       "7198  2010-09-29 16:43:00.000000172     9.332487\n",
       "7322   2011-10-04 09:37:00.00000077     7.654973\n",
       "7526   2011-03-06 21:01:00.00000081    11.478248\n",
       "7607  2012-01-26 07:33:00.000000136     8.819078\n",
       "7610   2012-01-26 07:33:00.00000023     8.758911\n",
       "7637   2010-12-09 07:29:00.00000083     8.853547\n",
       "7839    2012-12-15 06:35:45.0000001    14.201680\n",
       "7874    2009-06-02 08:34:33.0000001     8.591836\n",
       "7994    2013-01-19 20:19:45.0000002    10.114151\n",
       "8159    2011-07-20 08:05:02.0000001     9.613750\n",
       "8381    2009-07-30 15:49:15.0000002    18.177299\n",
       "8424  2009-06-10 16:55:00.000000155     9.628827\n",
       "8426  2011-06-24 12:03:00.000000131     8.957011\n",
       "8517   2011-03-11 01:44:00.00000067     9.817891\n",
       "8596   2010-09-20 16:48:00.00000021     8.817124\n",
       "8632    2009-10-29 10:48:40.0000003     8.977029\n",
       "8835    2012-12-01 21:12:12.0000001     9.632716\n",
       "9006    2013-07-02 22:27:14.0000001    11.935193\n",
       "9221  2010-08-27 18:45:00.000000215     8.500797\n",
       "9383   2011-03-11 01:44:00.00000064     8.998912\n",
       "9421   2011-10-04 09:37:00.00000088     7.518601\n",
       "9478   2011-12-13 22:00:00.00000044    12.372449\n",
       "9500   2014-07-21 18:19:00.00000025     7.751128\n",
       "9563   2011-03-06 21:01:00.00000018     8.575669\n",
       "9830   2014-07-21 18:19:00.00000065     7.517757\n",
       "9888  2013-09-25 22:00:00.000000146     7.461161\n",
       "4080    2010-06-11 13:37:21.0000004    44.258225\n",
       "5887    2010-07-04 16:44:11.0000002    42.257711\n",
       "8529    2009-11-25 19:32:52.0000001    25.599010\n",
       "\n",
       "[9914 rows x 2 columns]"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_submit(y_test, 'rf_model.pkl', 'dl_distance_none_model.json', 'lg_model_2.pkl', keras=False, keras_distance_none=True, keras_distance_high=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub = X[0:150000]\n",
    "y_sub = y[0:150000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def lightgbm(X, y, distance_none=False):\n",
    "    \n",
    "    X_b, X_test_b, y_b, y_test_b = train_test_split(X,y)\n",
    "    \n",
    "    d_train = lgb.Dataset(X_b, label=y_b)\n",
    "    params = {}\n",
    "    params['learning_rate'] = 0.03\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'regression'\n",
    "    params['metric'] = 'rmse'\n",
    "    params['sub_feature'] = 0.8\n",
    "    params['num_leaves'] = 31\n",
    "    params['min_data'] = 18\n",
    "    params['max_depth'] = -1\n",
    "    #params['early_stopping'] = 500\n",
    "    params['subsample_for_bins'] = 200\n",
    "    params['subsample'] = 1,\n",
    "    params['subsample_freq'] = 1\n",
    "    params['reg_alpha'] = 5\n",
    "    params['reg_lambda'] = 10\n",
    "    params['min_split_gain' ]=0.5\n",
    "    params['min_child_weight']=1\n",
    "    params['min_child_samples']= 10\n",
    "    params['scale_pos_weight']=1\n",
    "    params['num_threads']=4\n",
    "    params['eval_freq']=50\n",
    "    params['colsample_bytree']=0.6\n",
    "    params['min_data']= 18\n",
    "            \n",
    "    lg_model = lgb.train(params, d_train, 27500)\n",
    "\n",
    "    y_pred = lg_model.predict(X_test_b)\n",
    "    rms = np.sqrt(mean_squared_error(y_test_b, y_pred))\n",
    "    print(rms)\n",
    "    \n",
    "    if distance_none:\n",
    "        joblib.dump(lg_model, 'lg_distance_none_model_2.pkl')\n",
    "        print('LightGBM model saved as \"lg_distance_none_model.pkl\"')\n",
    "    else:\n",
    "        joblib.dump(lg_model, 'lg_model_2.pkl') \n",
    "        print('LightGBM model saved as \"lg_model_2.pkl\"')\n",
    "        \n",
    "    return lg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7358241871912417\n",
      "Linear Regression model saved as \"lg_model_2.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1a3fa64e48>"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightgbm(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.926970467670571\n",
      "Linear Regression model saved as \"lg_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x106e63358>"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightgbm(X_distance_none, y_distance_none, distance_none=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8254572341080313\n",
      "Linear Regression model saved as \"lg_model_2.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1a75e33710>"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightgbm(X_sum, y_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  rmse scores: [12.02188153 11.36308207]\n",
      "Random Forest  mean score: 11.692481801368658\n",
      "Random Forest  std: 0.32939972786696803\n",
      "Random Forest model saved as \"rf_distance_none_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=16, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=3,\n",
       "           min_samples_split=12, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=375, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest(X_distance_none, y_distance_none, distance_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  rmse scores: [37.25236114 50.27343311]\n",
      "Random Forest  mean score: 43.76289712268644\n",
      "Random Forest  std: 6.510535986444008\n",
      "Random Forest model saved as \"rf_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=16, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=3,\n",
       "           min_samples_split=12, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=375, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest(X_distance_high, y_distance_high, distance_high=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  rmse scores: [2.82355583 2.80782918]\n",
      "Random Forest  mean score: 2.8156925063124\n",
      "Random Forest  std: 0.007863325819166267\n",
      "Random Forest model saved as \"rf_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=16, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=3,\n",
       "           min_samples_split=12, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=375, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  rmse scores: [2.82402096 3.35434872]\n",
      "Random Forest  mean score: 3.089184840584704\n",
      "Random Forest  std: 0.2651638777896661\n",
      "Random Forest model saved as \"rf_sum_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=16, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=3,\n",
       "           min_samples_split=12, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest(X_sum, y_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  rmse scores: [12.01854581 11.35106257]\n",
      "Random Forest  mean score: 11.684804191997772\n",
      "Random Forest  std: 0.3337416186447584\n",
      "Random Forest model saved as \"rf_distance_none_500_model.pkl\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=16, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=3,\n",
       "           min_samples_split=12, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest(X_distance_none, y_distance_none, distance_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would have liked more time in the competition to run hyperparamater tests with LightGBM, and to add more features. LightGBM and Random Forests delivered the best overall scores, while Ridge did very well with the high distance dataset and Deep Learning did quite well overall. I still want to run tests on all the data, and resample the distance_high and distance_none phenomenon to obtain better results. Although nowhere near commercial, approaching an RMSE of 2.50 is not bad. New Yorkers, who would likely have a lot more detail as what can be feature engineered, and more time and rides to run through deep learning tests would likely produce better resuls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
